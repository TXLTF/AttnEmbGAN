分类号TP391.4学校代码10495ＵＤＣ004.8密级公开硕士学位论文基于残差注意力网络的多针迹刺绣图像生成算法研究作者姓名：杨辰学号：2015363088指导教师：胡新荣教授学科门类：工程专业：软件工程研究方向：计算机视觉完成日期：二零二三年六月WuhanTextileUniversityM.E.DissertationMulti-StitchEmbroideryImageGenerationviaResidualAttentionNetworkCandidate：YangChenSupervisor：Prof.HuXinrongTime:June2023摘要刺绣作为一个古老的艺术形式，其相关的图像生成工作一直广受工业界和学界的关注。现实中的刺绣，往往会具有鲜亮的色彩、复杂的纹理和种类多样的针迹。这使得刺绣图像生成是一个具有挑战性的课题。然而，使用现有的传统方法和基于生成对抗网络的方法所生成的刺绣图案，会出现色偏、纹理杂乱以及失去原有结构等问题。同时，其中基于生成对抗网络的方法并没有考虑刺绣针迹对刺绣的影响。围绕上述问题，本文对多针迹刺绣图案生成任务展开研究：（1）本文提出了一种基于残差注意力网络的刺绣图像生成网络框架。受到残差注意力网络的启发，针对刺绣生成任务设计了三种注意力掩码，分别是色彩注意力掩码、纹理注意力掩码、源注意力掩码。在非成对数据集条件下，该网络能够生成刺绣的色彩和纹理并进行融合，避免刺绣结果图出现色偏、纹理杂乱甚至失去输入图的结构等问题。（2）同时，本文对上述网络做出进一步优化，提出了基于残差注意力网络的多针迹刺绣图像生成网络框架。通过添加一个多针迹模块，大幅度增强了结果图中针迹风格的多样性。提出的多针迹模块可根据色彩区域的形状特征匹配合适的针迹类型，从而生成具有多种针迹风格的刺绣图，使结果进一步接近真实的刺绣。同时在研究过程中，发现了一种白色填充技巧，大幅提高预测阶段中网络生成刺绣纹理的稳定性，解决了生成过程中偶尔出现的无法正常生成纹理的问题。（3）在研究过程中制作了一个多针迹刺绣数据集。该数据集包含成对的参考图和刺绣图，每一张刺绣图标记有一种刺绣针迹类型标签。目前该数据集包括榻榻米针迹、平针针迹、缎纹针迹三种针迹。据调研，该数据集不仅是首个公开的标有针迹类型的刺绣数据集，同时也是目前规模最大的刺绣图像数据集。大量的定性实验和定量实验均可表明，本文提出的网络框架生成的刺绣图像，优于现有的方法。在用户研究和对比实验中，本文刺绣结果的色彩更接近输入图，刺绣纹理更加逼真，纹理包含了三种针迹风格。本文算法生成的刺绣图，在弗雷歇起始距离（FID）和可学习感知图像块相似度（LPIPS）两种度量评估的分数均小于其他方法，说明其刺绣图像分布更接近真实的刺绣图像分布。关键词：多针迹刺绣生成；风格迁移；生成对抗网络；残差注意力网络研究类型：应用基础研究AbstractAsanancientartform,therelatedworkofembroideryimagesynthesishasbeenwidelyfollowedbytheacademic.Realembroideryimageoftenboastsvibrantcolors,intricatetextures,anddiversestitchtype.Thismakesembroiderysynthesisachallengingtask.However,existingtraditionalmethodsandthosebasedonGenerativeAdversarialNetworks(GANs)forgeneratingembroideryimagessufferfromproblemssuchascolorshift,chaotictextures,andlossoforiginalstructure.Additionally,previousworksbasedonGANsdidnottakeintoaccounttheimpactofstitchtypesonembroidery.Therefore,thispapermainlyexploresandstudiesthemulti-stitchembroiderysynthesis:(1)Thispaperproposesanetworkframeworkforembroideryimagegenerationviaresidualattentionnetwork.Basedontheideaofresidualattention,threeattentionmasksaredesigned,namelycolorattentionmask,textureattentionmaskandsourceattentionmask.Undertheunpaireddatasets,thenetworkcangenerateembroiderycolorandtextureimagesandfusethem.Soastoavoidtheproblemsofcolorcast,messytextureandevenlosingthestructureoftheinputpictureinthegeneratedembroiderypicture.(2)Thispaperfurtherupdatetheabovenetworkandproposesanetworkframeworkformulti-stitchembroideryimagegenerationviaresidualattentionnetwork.Byaddingamulti-stitchmodule,thediversityofstitchstylesintheresultsisgreatlyenhanced.Theproposedmulti-stitchmodulecanmatchtheappropriatestitchtypesaccordingtotheshapecharacteristicsofcolorregions,makingtheresultshavevariousstitchstylesandclosertotherealembroidery.Intheresearchprocess,awhitefillingtechniquewasfound,whichgreatlyimprovedthestabilityofembroiderytexturegeneratedbynetworkinthepredictionstage.Itsolvestheproblemthatthetexturecannotbegeneratednormallyoccasionallyinthegenerationprocess.(3)Amulti-stitchembroiderydatasetwasproducedinthispaper.Thisdatasetconsistsofpairsofreferenceimagesandcorrespondingembroideryimages,witheachembroideryimagebeinglabeledwithatypeofembroiderystitchtype.Currently,thedatasetincludesthreestitchtypes:tatamistitch,flatstitch,andsatinstitch.Accordingtoresearch,thisdatasetisnotonlythefirstpubliclyavailableembroiderydatasetthatislabeledwithstitchtypes,butalsothelargestembroideryimagedatasetcurrentlyavailablefornetworklearning.Extensivequalitativeandquantitativeexperimentsdemonstratethattheembroideryimagesgeneratedbytheproposednetworkframeworkinthispaperaresuperiortoexistingmethods.Inuserstudyandcomparativeexperiments,thecolorofresultsisclosertotheinput,andtheembroiderytextureismorerealistic.Andthetexturecontainsthreestitchstyles.Inquantitativeexperiments,theresultsinthispaperhaslowerscoresinFréchetInceptionDistance(FID)andLearnedPerceptualImagePatchSimilarity(LPIPS)thanothermethods,whichshowsthattheembroideryresultsdistributionisclosertotherealembroideryimagedistribution.Keywords:Multi-stitchembroiderygeneration;Styletransfer;Generativeadversarialnetworks;ResidualattentionnetworkThesis:Appliedbasicresearch目  录 1 绪论 ......................................................................................................... 1 1.1 研究背景及意义 ............................................................................... 1 1.2 国内外的研究现状 ........................................................................... 1 1.2.1 基于图像的传统生成方法 ......................................................... 2 1.2.2 基于深度学习的生成方法 ......................................................... 2 1.3 论文的主要工作和研究内容 .......................................................... 4 1.4 论文的组织结构 ............................................................................... 5 2 相关知识与理论基础 ............................................................................ 6 2.1 三种基础刺绣针迹 ........................................................................... 6 2.2 刺绣数据集 ....................................................................................... 6 2.2.1 单针迹刺绣数据集 ..................................................................... 7 2.2.2 多针迹刺绣数据集 ..................................................................... 7 2.3 基于传统方法的刺绣图像生成 ...................................................... 8 2.4 生成对抗网络的原理与特点 .......................................................... 9 2.4.1 基本原理 ..................................................................................... 9 2.4.2 网络特点 ................................................................................... 10 2.5 基于生成对抗网络的图像生成 .................................................... 10 2.5.1 pix2pix ........................................................................................ 10 2.5.2 CycleGAN .................................................................................. 11 2.5.3 MUNIT ....................................................................................... 12 2.5.4 DRIT .......................................................................................... 12 2.5.5 Pixel2Style2Pixel ....................................................................... 13 2.5.6 AttentionGAN ............................................................................ 14 2.6 本章小结 ......................................................................................... 14 3 基于残差注意力的刺绣图像生成 ...................................................... 16 3.1 引言与研究动机 ............................................................................. 16 3.2 算法相关的前置工作 ..................................................................... 17 3.2.1 残差模块 ................................................................................... 17 3.2.2 残差注意力网络 ....................................................................... 17 3.3 基于残差注意力的刺绣图像生成算法 ........................................ 18 3.3.1 网络结构 ................................................................................... 18 3.3.2 损失函数 ................................................................................... 20 3.3.3 算法流程 ................................................................................... 20 3.4 本章小结 ......................................................................................... 21 4 基于多针迹模块的刺绣图像生成 ...................................................... 22 4.1 引言与研究动机 ............................................................................. 22 4.2 算法相关的前置工作 ..................................................................... 22 4.2.1 边缘填充操作 ........................................................................... 22 4.2.2 各针迹区域的形状特征 ........................................................... 23 4.3 基于多针迹模块的刺绣图像生成算法 ........................................ 23 4.3.1 网络结构 ................................................................................... 23 4.3.2 损失函数 ................................................................................... 26 4.3.3 算法流程 ................................................................................... 28 4.4 本章小结 ......................................................................................... 29 5 实验设计与结果分析 .......................................................................... 30 5.1 实验设置 ......................................................................................... 30 5.1.1 数据集预处理 ........................................................................... 30 5.1.2 实现细节 ................................................................................... 30 5.2 评估方法 ......................................................................................... 30 5.2.1 定性评估方法 ........................................................................... 30 5.2.2 定量评估方法 ........................................................................... 31 5.3 对比实验 ......................................................................................... 32 5.3.1 超参数调优实验 ....................................................................... 32 5.3.2 定性对比实验 ........................................................................... 32 5.3.3 定量对比实验 ........................................................................... 35 5.4 消融实验 ......................................................................................... 35 5.4.1 多针迹模块的消融实验 ........................................................... 35 5.4.2 相关损失函数的消融实验 ....................................................... 36 5.4.3 白色填充技巧的消融实验 ....................................................... 37 5.5 本章小结 ......................................................................................... 37 6 总结以及展望 ...................................................................................... 39 6.1 总结 ................................................................................................. 39 6.2 展望 ................................................................................................. 40 参考文献 ................................................................................................... 41 1绪论11绪论1.1研究背景及意义刺绣是一种古老的艺术形式，使用针和线在织物上绣制各种装饰性图案，捕捉各种美丽图案的色彩特征。然而，手动创建刺绣图像是一项费力且需要大量手工技能的工作，因此自动刺绣图像生成是一项具有挑战性和价值的任务。在设计刺绣时，艺术家需要考虑参考图像的颜色信息，并缝制合适的刺绣纹理。经过调查研究，高质量的刺绣图像需要满足三个基本条件：（1）刺绣的颜色应尽可能接近参考图像的颜色；（2）刺绣的纹理应该是均匀，整洁和多样化的；（3）一幅刺绣往往包含多种针迹。在刺绣艺术中，利用不同的针迹展现出具有纹理特征和图案的多彩形态的织物结构。针迹由缝纫针以不同的速度、质量和复杂度在各个层次上编织交叉的线或纱而成。由此制成的刺绣图案是美丽而复杂的。随着机器学习的发展，从图像中训练计算机识别和再现刺绣图案变得可能、必要和有用。然而，专业的刺绣中使用了数十种针法，有些针法非常相似，这使机器学习变得复杂。目前，消费者希望将计算机运用在他们的刺绣图案定制项目中，并实现个性化的风格。作为纺织品生产的工匠，这些人希望能够自动高效地依据参考图生成对应刺绣图，以便能够将更多的时间专注于他们的创意作品。虽然艺术形式的图像风格化已经得到广泛研究，但是刺绣生成的风格化仍然是一个重要挑战。传统的刺绣生成方法[1,2]使用传统算法渲染刺绣纹理，这使得它耗时且费力，并且生成的刺绣纹理不够真实。最近，基于学习的刺绣生成方法[3,4]在图像可以基于提供的输入图像进行风格化方面引起了相当大的关注。近年来，尽管基于学习的风格化方法取得了显著的成功，包括图像到图像的转换[5-8]以及注意力指导的图像到图像的转换[9]，但它们通常无法生成令人满意的刺绣图像。造成这种情况的原因有三个。首先，刺绣图像的颜色需要尽可能接近参考图像的颜色，使得没有相关约束的方法会导致颜色偏移的问题。其次，刺绣纹理过于复杂和特殊，这使得这些方法生成的刺绣纹理太杂乱甚至消失。最后，往往一幅刺绣具有多种针迹，但生成具有针迹特征鲜明的刺绣对于现有基于学习的方法十分困难。如何有效地解决上述问题，是多针迹刺绣生成的难题。本文旨在研究效果更好的多针迹刺绣生成算法，实现色彩接近参考图，纹理接近真实刺绣，包含多种针迹的刺绣风格转换。1.2国内外的研究现状近年来，刺绣图像生成在国内外被广泛研究。从早期基于图像的传统方法到如今的深度学习方法，各个时期都有其大量的研究典例。武汉纺织大学硕士学位论文21.2.1基于图像的传统生成方法在刺绣图像生成的领域，最具有挑战的部分，是需要同时让刺绣的色彩接近参考图，刺绣的纹理接近真实刺绣。经过训练的艺术家能够快速捕捉参考图的色彩特征，通过比对找出最接近参考图的色彩。同时，艺术家们依据区域不同的形状特征，设计出多种针迹，这些针迹决定了刺绣的纹理风格。将刺绣艺术家这种能力迁移到计算机上，依据输入的参考图渲染出对应的刺绣图，便是早期的计算机方法——基于图像的传统生成方法。最初的传统方法，仅仅考虑一种针迹。Chen等人[10]提出了一种从线条图案自动生成刺绣图案的技术。Cui等人[11]将基于笔画的渲染技术与Phong光照模型相结合，解决了之前Chen等人工作[10]中存在的不连贯的阴影问题。在刺绣的光照方面，Shen等人[12]提出了一种保持照明的刺绣模拟方法。而乱针绣是一种更为复杂的风格。Yang等人[13]提出了一种多层渲染技术来生成乱针绣纹样，而Zhou等人[14]为每个区域添加了新的针迹属性。之后的工作逐渐开始关注生成图像中的针迹风格。例如，Yang等人[15,16]关注了针迹构造的本质，揭示了与乱针绣相关的不同艺术风格。Qian等人[17]向灰度图像添加噪声并应用风格转移方法以改善模型训练。Chen等人[18]提出了一种使用马尔科夫链模型的乱针绣纹样生成器。尽管这些方法已经有了一定的进展，但它们太过依赖用户设置图像分割数据和针迹风格参数[19]，使得它们费时且劳动密集。而且，这些方法不够自动化，无法让非领域专家使用它们来解释和创建刺绣设计。近年来，一些方法试图解决这些问题。Liu等人[2]提出了一种新的基于曲线的方法来程序化地生成复杂的刺绣，该方法可以采用由线条笔划表示的2D视觉设计并产生可渲染的曲线。Guan等人[1]提出了一个基于自动纹理生成的框架，将任意输入图像转换成刺绣风格的艺术，用于服装设计和在线显示。值得注意的是，这项工作关注了刺绣针迹对于刺绣纹理风格质量的影响，开始尝试依据色彩区域的形状特征匹配适合的刺绣针迹类型，生成对应的刺绣纹理。然而，由于传统刺绣生成方法的局限性，使用该方法生成的刺绣纹理单调，与真实刺绣相差较大。1.2.2基于深度学习的生成方法（1）基于深度学习的刺绣生成近几年，开始出现基于深度学习的刺绣图像生成方法，目前这些方法主要生成单针迹刺绣。Qian等人[20]提出了一个包含两个损失函数的CNN来生成乱针绣图像，Wei等人[4]使用语义分割方法从刺绣综合过程中提取目标内容图像。由于上述针迹风格是随机的，不能用来检查多个针迹构造的组合。也就是说，乱针绣只能被拆解为单针成分风格。Beg等人[3]提出了一种无监督的图像到图像转换方法来生成刺绣图像，但它也呈现出单针特征。因此，我们选择关注不同针迹类型的组合对生成刺绣图像的质量和复杂度的影1绪论3响。（2）图像到图像转换的通用网络框架图像到图像的转换是从某个图像域映射到另一个图像域的工作，现已存在大量的通用框架。生成对抗网络（GAN）在图像到图像的翻译方面取得了显著进展[21]。在此基础之上，已经有了大量成熟的工作[22,23]提供了图像到图像翻译问题的解决方案。其中Pix2Pix[23]是基于条件GAN[24]的通用框架。然而，获得足够的成对刺绣图像数据集具有挑战性，这些方法需要使用成对的图像数据集进行训练，这是刺绣风格化的困难之处。为了解决以上问题，CycleGAN[25]是最先提出的非成对的学习两个域之间的图像转换，通过引入循环一致性损失函数而不需要成对的数据。StyleGAN[26,27]可以在潜在编码层面上修改生成的面部信息，但它在刺绣生成任务中无法获得良好的结果。UNIT模型[7]是基于不同数据空间共享相同潜在空间的假设而设计的。因此，可以将图像到图像转换问题视为潜在空间转换。MUNIT[5]将共享的潜在空间定义为内容空间，差异潜在空间是用于执行多模态图像到图像转换的样式空间。一些工作[28,29]使潜码服从高斯分布，使得网络更便于预测潜码信息。之后其他工作[30]增加了基于变分自编码器（VAE）的条件信息，同时Bao等人[31]在其基础上添加了GAN模块。多样的图像到图像表征分离转换（DRIT++）[32]也使用VAE来解决分离表示的任务。然而，这些方法不仅无法生成具有多针迹风格的刺绣图像，并且它们处理图案复杂性的能力较差。换句话说，这些方法生成的刺绣图色彩和纹理均无法令人满意。色彩上与原图差距较大，也就是提到的出现色彩偏移的问题。同时，纹理作为刺绣图像的高频特征是本文的关键学习目标，但上述方法生成的刺绣结果图与真实刺绣图差距较大，而且常常会出现生成纹理失效的问题。（3）基于注意力机制的图像到图像转换为了克服这些问题，本研究继而将目标转向基于注意力机制的图像到图像转换的方法。注意力机制已成功地引入了许多计算机视觉应用，例如深度估计[33]，帮助模型专注于输入的相关部分以解决相应的输出，而无需任何监督。受此启发，一些工作使用注意模块以一种无监督的方式关注图像翻译任务的感兴趣区域，可以分为两类。第一类方法是使用额外的数据提供注意力机制。例如，Liang等人提出了ContrastGAN[34]，它使用每个数据集的对象掩码注释作为额外的输入数据。Sun等人[35]使用全卷积网络生成的掩码编辑面部属性。此外，InstaGAN[36]结合实例信息（例如对象分割掩码）并改进了多实例转换。第二类是训练另一个分割或关注模型来生成注意力映射并将其拟合到系统中。例如，Chen等人[37]使用额外的注意力网络生成注意力映射，以便更多关注可以放在感兴趣的对象上。Kastaniotis等人提出了ATAGAN[38]，它使用知识蒸馏生成注意力映射。Yang等人[39]建议添加一个注意力模块进行预测。Zhang等人提出了SAGAN[40]用于图像生成任务。Kim等人[41]建议使用辅助分类器生成注意力蒙版。Mejjati等人[42]提出了与生成武汉纺织大学硕士学位论文4器、鉴别器和其他两个注意力网络联合训练的注意力机制。然而，上述方法都需要使用额外的网络或数据来获得注意力掩码，这增加了整个系统的参数数量、训练时间和存储空间。为了解决此问题，Tang等人提出了AttentionGAN[9]，将注意力方法嵌入到原始生成器中，因此不需要任何额外的模型来获取感兴趣物体的注意力掩码。值得注意的是，该方法也可以用于多域图像到图像转换的任务[43]中。注意力机制指导的图像到图像转换遵守一些由外部指定的约束条件，例如类别标签[44,45]，文本描述[46,47]，人体关键点和骨骼[48,50]分割地图[51-54]和参考图像[55,56]。由于不同的生成任务需要不同的指导信息，现有的工作针对特定应用进行了定制，即使用特别设计的网络架构和训练目标。例如，Ma等人提出了PG2[48]，它是一个两阶段框架，使用姿势掩码损失来生成基于人物图像和人体姿势关键点的人物图像。Tang等人提出GestureGAN[49]，它是一个前向-后向一致性架构，采用所提出的颜色损失来生成基于输入图像和条件手骨架的新手势图像。Wang等人提出了Vid2Vid框架[51]，它使用一个精心设计的权重生成模块来生成视频，真实地反映输入图像的风格和条件分割地图的布局。注意力学习在计算机视觉和自然语言处理中已经得到广泛应用[58,59]。为了提高图像生成性能，注意力机制最近也在图像到图像转换任务中进行了研究[60]。1.3论文的主要工作和研究内容本文主要探索和研究基于残差注意力网络的非成对多针迹刺绣图像生成。随着深度学习和生成对抗网络的日益发展，本文将多针迹刺绣图像生成过程定义为从参考图域到刺绣图域的一种映射。之前从CycleGAN[6]框架延伸出的刺绣图像生成算法[3]，存在刺绣结果图的色彩与参考图的色彩差距较大，纹理不够真实等问题。而且基于深度学习生成的刺绣图无法像传统方法那样拥有多种针迹。同时，成对的刺绣图像数据获取难度较大。上述问题使得非成对的多针迹刺绣图像生成具有很大的挑战性。受到AttentionGAN[9]的启发，本文提出了基于残差注意力网络的刺绣图像生成算法AttnEmbGAN（UnpairedEmbroideryGenerationUsingEmbroideryResidualAttention）。该算法通过残差注意力模块解耦刺绣图像的色彩表示和纹理表示来进行建模，通过色彩注意力、纹理注意力、源图注意力三种注意力掩码生成刺绣图像。通过大量的定性和定量实验，充分展示了本文算法的结果明显优于现有的几种主流方法。为了增强刺绣针迹类型的多样性，本文改进了AttnEmbGAN，提出了基于残差注意力网络的多针迹刺绣图像生成算法AttnMSEmbGAN（UnpairedMulti-StitchEmbroiderySynthesisUsingEmbroideryResidualAttention）。该算法将AttnEmbGAN中的纹理注意力掩码进行了进一步的分割（平针针迹纹理注意力、缎纹针迹纹理注意力和榻榻米针迹纹理注意力）。并且设计了一个多针迹模块，在网络的预测阶段辅助网络生成具有多种1绪论5针迹纹理的刺绣纹理图。定性和定量地对比现流行的几种风格迁移算法和AttnEmbGAN，该算法不仅保留了原AttnEmbGAN生成的刺绣图像的优点，同时多种针迹纹理生成的能力得到了大幅增强。1.4论文的组织结构本文围绕基于残差注意力网络的多针迹刺绣图像生成算法展开研究。总共分为以下六章：第1章，绪论。围绕研究背景，分析了国内外刺绣图像生成的研究现状。介绍工作内容和组织结构。第2章，展开介绍了多针迹刺绣图像生成相关的理论基础。首先是一个公开的多针迹刺绣图像数据集，介绍了其制作流程，这也是此项工作中的一个贡献。然后，介绍了基于传统方法的刺绣图像生成。同时，由于本文的多针迹刺绣图像生成使用了生成对抗网络框架，因此需要深入分析生成对抗网络的基础内容。最后介绍了目前流行的几种生成对抗网络的通用框架。第3章，提出了基于残差注意力网络的刺绣图像生成算法AttnEmbGAN。先分析了现有工作的问题，基于刺绣本身的特点，启发设计新的框架对刺绣图像进行针对性的非成对的刺绣图像生成。然后介绍了新算法的前置知识，包括残差模块、残差注意力网络。详细介绍了基于残差注意力网络的非成对刺绣图像生成算法AttnEmbGAN的相关内容。第4章，在AttnEmbGAN的基础上，提出增强了刺绣针迹类型的基于残差注意力网络的多针迹刺绣图像生成算法AttnMSEmbGAN。先分析了第3章的AttnEmbGAN算法在生成包含多种针迹纹理刺绣图任务中存在的缺陷。然后介绍了新算法的前置知识，包括边缘填充操作、各针迹区域的形状特征。之后详细介绍了提出的同样基于残差注意力网络的多针迹刺绣图像生成算法AttnMSEmbGAN的相关内容及白色填充技巧。第5章，实验结果的展示和分析。先介绍了AttnEmbGAN和AttnMSEmbGAN算法实验的数据集预处理和代码实现细节。现有流行的定性和定量的分析方法。最后是实验结果（定性对比实验、定量对比实验、消融实验）的展示和分析。第6章，本文的总结和展望。首先对本文进行总结，之后进一步展望未来的研究和工作方向。武汉纺织大学硕士学位论文62相关知识与理论基础围绕本文提出的基于残差注意力网络的多针迹图像生成算法，本章对其相关知识和理论基础进行介绍。首先，介绍了三种基础的刺绣针迹。其次，介绍现有的已公开的刺绣数据集和本工作中制作的全新的多针迹刺绣数据集。之后，介绍了生成对抗网络的相关基础。最后，展开讨论现流行的几种基于生成对抗网络的图像生成方法。2.1三种基础刺绣针迹图2.1三种基础针迹Figure2.1Thethreebasicstitches刺绣艺术通常由各种针迹和表现为刺绣图案的颜色形式组成。制作刺绣需要技术和审美能力。针迹类型直接决定了最终纹理效果的特征。本研究的重点是研究多针迹刺绣模型和其对应的数据集，因为现有针迹多达几十种，因此在研究中使用三种基础针迹（缎纹针迹，榻榻米针迹和平针针迹），如图2.1所示。图中小图是针迹的走针线路，大图是渲染出来的对应的刺绣纹理。这三种针法的特点描述如下：缎纹针迹是一种非常常见的刺绣针迹。针头沿物体轮廓两侧落下，折叠的线条呈蛇形移动。缎纹针的角度、密度和长度因物体而异。榻榻米针迹，也称为席纹针，类似于榻榻米草编织纹路。刺绣针法形成一个块状表面，遵循线性行进路径，形成统一有序的针群。平针针迹是另一种常见的刺绣针迹，其中针沿着线性路径连续缝制，形成具有不同厚度和形状表现的图案。2.2刺绣数据集多针迹刺绣图像生成需要大量的刺绣图像数据，并且不需要对每张刺绣图标记对应的刺绣针迹。首先，不同针迹类型的刺绣图，其纹理风格差别是十分巨大的。其次，刺绣图像数据获取成本昂贵，现有公开的刺绣数据集很少，且规模较小。并且没有标记针迹类型的刺绣数据集。因此本研究过程中，制作了首个标记了针迹类型的刺绣数据集——多针迹刺绣数据集，同时也是目前规模最大的刺绣数据集（图片总数超过30000张）。2相关知识与理论基础72.2.1单针迹刺绣数据集刺绣图像数据可以分为单针迹图像数据和多针迹刺绣图像数据。这主要取决于刺绣图像是否标记了对应的针迹类型。目前现有的刺绣数据集，是一个包含有9000组输入草图和相应的输出刺绣图像的成对的刺绣数据集[61]，但是这类数据集依然具有其局限性。在传统的刺绣生成方法中，刺绣的针迹类型逐渐被重视，多针迹刺绣图像的生成已经开始流行。但由于现存的所有刺绣数据集，均没有标记刺绣的针迹信息。因此，基于深度学习的刺绣生成方法，目前仅仅只能进行简单的端到端的刺绣风格转换工作，无法用于较为复杂的多针迹刺绣图像生成。2.2.2多针迹刺绣数据集在本文研究中，为社区贡献了一个多针迹刺绣数据集。多针迹绣数据集有一些很好的属性。首先，这种刺绣的针迹是由经验丰富的刺绣设计师设计的，这意味着每个区域匹配的针迹类型都是最符合其区域形状特征的。其次，多针迹绣数据集使用了三种基础针迹类型[62]进行标记，包括缎纹针迹、榻榻米针迹和平针针迹。其三，该数据集规模超过30000张参考图与刺绣图数据，包含成对的和非成对的两部分。其四，刺绣图案由专业的刺绣设计软件渲染。最后，数据集中的图像没有光影等冗余信息干扰模型的学习，同时也更便于后期的二次渲染。图2.2多针迹刺绣数据集Figure2.2Multi-stitchEmbroideryDataset如图2.2所示，多针迹刺绣数据集中的数据被标记为四种标签，分别对应于三种基础针迹类型（缎纹针迹、榻榻米针迹、平针针迹）和一种混合了三种单针类型的多针迹类型。数据集包括成对的和非成对的部分，成对的部分包括参考图和对应的刺绣图。多针迹刺绣数据集的刺绣图案由刺绣设计师设计，并通过专业刺绣软件渲染。每个区域都与最适合的刺绣类型相匹配。为确保模型能够学习输入图像的相应特征，没有选择过于复杂的图案。在数据集中，刺绣图案设置为单一的刺绣类型和单一的颜色。缝合数据的部分应尽可能平均。最终，将获得一组参考/刺绣图像对。以下是刺绣图案制作的武汉纺织大学硕士学位论文8步骤：（1）绘制参考图案在进行刺绣版制作之前，先需要绘制带有刺绣颜色信息的参考图，作为刺绣图像制作的模板。数据集中的内容图像应包括形状信息和色彩信息。数据集中的大部分内容图像都足够简单且色彩清晰。具有简单特征的刺绣图像可以使网络更快地收敛。同时，还挑选了部分具有复杂形状和颜色的图片，这使得网络可以处理一些复杂的图像。（2）针迹设计对于不同区域形状的参考图案，设计师填充一种与之匹配针迹纹理。不同宽度的形状适合不同的针织样式。对于针织类型的参数设置，我们尽可能保持一致性，以便于提取纹理特征。（3）依据针织类型进行多层渲染使用专业的刺绣版制作软件进行渲染和生成刺绣模拟图像。其分辨率经过调整，能够清晰地表示针迹的细节。因此，在训练过程中，当网络对数据集进行局部采样时，仍然可以保留刺绣纹理的细节。此外，当使用基于全卷积的网络进行训练时，也意味着可以获得其他分辨率的结果。图2.3多针迹刺绣数据集的分布Figure2.3DatadistributionoftheMulti-stitchEmbroideryDataset多针迹刺绣数据集包含超过30000的高质量刺绣图像，包括超过13000的对齐内容刺绣图像和超过17000的未对齐图像，具体分布如图2.3所示。2.3基于传统方法的刺绣图像生成在介绍基于深度学习的方法之前，还需简单了解基于传统方法的刺绣图像生成。由于现有的刺绣数据集规模较小，针迹类型信息的缺失，这导致基于生成对抗网络的刺绣图像生成的结果不一定优于最新的传统刺绣图像生成方法[1]。2相关知识与理论基础9传统的刺绣图像生成方法往往可以分成下面几个大类：（1）基于图块(patch)的方法基于图块的方法首先将输入的图像分割成多个图块或超像素。之后使用生成算法进行邻接选择和重构权重。可见传统的刺绣图像生成算法存在成本高昂和细节缺失（块效应和模糊效应）等问题。（2）基于图像(image)的方法相比于注重局部信息的基于图块的方法[63]，基于图像的方法更好地利用了全局信息。基于图像的方法首先将整个图像作为输入，直接进行像素级的生成。依据输入图像的色彩和区域的形状，使用匹配的刺绣纹理进行填充。（3）基于笔画(stroke)的方法在刺绣生成领域中，有时是以线条笔画图作为刺绣图案的参考图作为输入。每种算法都要根据其对应的针迹风格进行专门设计，值得注意的是，不同针迹风格的刺绣可能需要不同的分配方法。有部分方法需要依赖于用户交互来确定针迹风格。其中每种针迹需要设计一个对应的算法，这极大的提高了使用成本。生成的图像质量相比前两种方法更好，但是算法复杂度也高很多。2.4生成对抗网络的原理与特点本文研究的非成对刺绣图像生成基于残差注意力网络指导的生成对抗网络框架，因此该小节简单介绍生成对抗网络的基本原理与应用。生成对抗网络（GenerativeAdversarialNetworks，GAN）[21]自被提出以来已愈加成熟，极大地促进了生成模型的发展。2.4.1基本原理深度学习中的生成对抗网络（GAN）包括两个组件：生成器（Generator）和判别器（Discriminator）。基于零和博弈理论的思想，使用对抗的方式训练生成器和判别器：生成器会被鼓励生成伪造样本()欺骗判别器；而判别器用于区分真实样本和伪造样本之间的差异。通过这种方式，生成器最终能够产生高度逼真的虚假样本，难以与真实样本区分。在训练过程中，GAN对生成器和判别器循环进行优化。（1）判别器的优化鼓励判别器准确地区分真实样本和生成样本，因此我们希望提升判别器的能力。对于真实样本∼()，我们想让判别器对其的判别准确度最大化，即log(())最大化。而对于生成样本()，提升判别器的能力，即log(1−(()))最大化。因此，判别器的目标函数可以表示为公式（2.1）：max,=∼log+∼(0,1)log1−#式2.1武汉纺织大学硕士学位论文10（2）生成器G的优化鼓励生成器生成近似真实的样本，同时干扰判别器分辨出真实样本和生成样本。我们想让生成器生成更真实的样本，判别器对()的判别准确度最小化，即log(1−(()))最小化。生成器的目标可以表示为公式（2.2）：min,=∼(0,1)log1−#式2.2结合上述公式（2.1）和公式（2.2），将生成对抗网络的生成器和判别器的博弈过程转为下述目标函数，见公式（2.3）：minmax,=∼log+∼(0,1)log1−#式2.3其中∼为训练数据，即真实样本，∼(0,1)是服从正态分布的随机噪声。()表示由生成器生成的样本，(·)是由判别器判断输入·为真实样本的期望。2.4.2网络特点尽管生成对抗网络在生成模型上做出了巨大贡献，但同时它也面临多个挑战。其中存在训练梯度爆炸、模式崩溃、收敛过难和梯度消失等问题。相对于传统模型，生成对抗网络可利用深度神经网络直接进行训练，同时导致其自由度过高。为了解决这些问题，研究者们不断对模型进行优化。生成对抗网络面临的另一个挑战是定性评价。目前现存的生成质量评价指标还不够令人满意。在图像生成任务中，评估许多模型的图像生成质量依然需要进行用户研究（UserStudy），需通过人的主观视觉评估进行定性评价。尽管FID、LPIPS等定量评价指标被广泛使用，但在某些任务上说服力仍然有限。因此，该部分还有很大的研究和改进的空间。随着生成对抗网络的不断发展和卓越的性能，已被广泛应用于各个领域。生成对抗网络最大的特点是能够生成接近真实样本的结果，现被广泛用于图像生成[64-66]、视频生成[67-69]、自然语言生成[70-72]、音乐创作[73-75]。此外，生成对抗网络还可用于数据增强。2.5基于生成对抗网络的图像生成本文研究探索基于残差注意力指导的生成对抗网络的多针迹刺绣图像生成，将图像数据分为两个域（参考图域和刺绣图域），换句话说，将刺绣图像生成定义为跨域图像生成。下文简单介绍现有的几种基于生成对抗网络的图像到图像的转换。2.5.1pix2pixpix2pix[23]作为首个用于图像到图像的转换的通用生成对抗网络框架，自提出以来，深受研究者们的关注。将条件生成对抗网络作为基础，pix2pix被提出。把传入生成器的条件编码从标签2相关知识与理论基础11信息替换为待转换的输入图的经过编码器下采样之后获得的潜码。同时，该模型使用的是成对的训练数据集，判别器用于判断成对数据的真假，如图2.4所示。图2.4pix2pix简介Figure2.4Overviewofpix2pix值得一提的是，pix2pix生成器中使用的U-net结构。作者通过这种跳层连接的网络结构，将输入图和输出图的底层信息进行共享，从而在图像到图像的转换过程中，使输入的结构接近输出的结构。2.5.2CycleGAN图2.5CycleGAN简介Figure2.5OverviewofCycleGAN在实际的应用过程中，往往很难获得大量的成对的输入图和目标图作为数据。话句话说，并不是每一张图像都有与之对应的另一个域的图像，或者有时某一个域的图像获取难度较大。例如，在一些图像转换任务中，任务目标是从普通马的照片（源图像）转换生成对应的斑马照片（目标图像）。但现实生活中，不可能存在一对除了身上条纹以外完全相同的普通马和斑马的照片。因此需要成对数据集的pix2pix具有天然的局限性，寻找一个可以使用非成对数据集的算法势在必行。为了解决上述问题，CycleGAN[25]被提出，能够使用不成对的数据集进行训练，进行图像到图像的转换生成。CycleGAN提出了“循环一致性”概念，该研究中提到，在图像到图像的转换过程中，需要保持“循环一致性”。具体来说，如图2.5所示，CycleGAN存在两个映射函数：→和：→，在现有的GAN的基础上添加一个与之对应的反向转换的GAN，共同组成一个具有循环结构的网络。显然，当域中的某个元素∼映射至域中的某个元素 ∼，之后再映射回域的某个元素 ∼时，元素 理论上应与元素一致。二武汉纺织大学硕士学位论文12者的距离定义为循环一致性损失。相应的，还有与之对应的→→的过程。网络训练的过程中不需要成对的数据集。CycleGAN的提出大幅度降低了数据集的获得门槛，扩大了图像转换的应用场景。其思想也影响了后续的很多研究。2.5.3MUNIT上述提到的图像转换方法均为一对一的生成过程，即一张输入图仅能获得一种风格的输出图。而实际应用场景中，图像生成往往需要一对多甚至多对多，即一张输入图获得多种不同风格的输出图。例如，从一张草图可以获得不同风格的靴子。为了满足上述需求，MUNIT[5]假设潜在空间是由内容空间和风格空间组成的。其中内容空间是共享的空间，具有相同内容信息的不同域的图像均可映射到内容空间上的同一个位置。而风格空间是独立的，拥有不同风格信息的图像会映射到不同的风格空间。通过对内容空间的内容编码和不同风格空间中的风格编码进行连接，可以从一张输入图获得多种风格的输出结果。图2.6MUNIT简介Figure2.6OverviewofMUNITMUNIT包含两个自动编码器，如图2.6所示。其中包含重构和编码交叉组合预测两个过程，将一致性作为约束。在无监督条件下，MUNIT能依据一张输入图生成多种不同风格的结果，在训练成本的降低和应用领域的扩展均做出了贡献。2.5.4DRITDRIT[32]以MUNIT的假设为基础，提出了另一种多模态图像生成算法。与MUNIT不同的是，DRIT提供了两种方法对两个空间中的潜码进行组合，拼接和元素级的特征变换。同时，添加了部分损失函数，对编码器生成的潜码的一致性进行约束。（如图2.7所示）2相关知识与理论基础13图2.7DRIT简介Figure2.7OverviewofDRITDRIT和MUNIT均可归类为表征解耦（disentangledrepresentation），其在多模态图像生成领域有着巨大的影响。2.5.5Pixel2Style2Pixel近几年，随着StyleGAN[27]的提出，由于其可编辑的特性，受到了越来越多的图像生成研究者的关注。Pixel2Style2Pixel[76]便是在StyleGAN的基础上提出的一个图像到图像的转换网络框架。图2.8Pixel2Style2Pixel简介Figure2.8OverviewofPixel2Style2Pixel在之前，先需简单介绍StyleGAN。不同于之前任务，潜在空间的噪声服从高斯分布。StyleGAN使用了一个映射网络将原潜在空间映射到了另一个潜在空间，服从一个更加均匀的属性分布。其最大的优势是空间与结果图的属性是线性关系，可以通过修改不同层级的潜码编辑结果图中对应的属性（例如人脸的五官，肤色，发型，背景）。武汉纺织大学硕士学位论文14而Pixel2Style2Pixel便是在这个基础上，使用标准特征金字塔提取特征图，传入StyleGAN中的不同层级（StyleGAN中网络的层级高低决定了其编辑的属性），如图2.8所示。Pixel2Style2Pixel将StyleGAN应用到了图像到图像转换的领域。2.5.6AttentionGAN目前，有大量工作将注意力机制应用于GAN，其中一些方法需要使用额外的网络或数据来获得注意力掩码，这将导致训练成本十分昂贵。基于这项问题，AttentionGAN[9]被提出，将注意力机制嵌入生成器中，分别生成注意力掩码和内容掩码，不需要额外的任何模型。图2.9AttentionGAN简介Figure2.9OverviewofAttentionGAN如图2.9所示，AttentionGAN由一个编码器和两个生成器组成。两个生成器分别直接生成内容掩码和注意力掩码，同时将注意力掩码继续分成前景注意力掩码和背景注意力掩码。之后对图像重新进行融合。整个网络框架同样遵循了CycleGAN提出的“循环一致性”概念。因此这项工作使用的数据集也是非成对的。AttentionGAN使用注意力机制区分前景和后景，然而刺绣生成中更注重色彩和纹理，因此AttentionGAN直接用于刺绣生成的结果并不理想。2.6本章小结本章主要阐述非成对多针迹刺绣生成的前置知识以及相关工作。首先介绍了刺绣的三种针迹，并且举例介绍了现有的刺绣图像数据集，同时介绍了本研究中新制作的数据2相关知识与理论基础15集的特点和优势。本研究的网络训练也需要基于该数据集。同时，由于本文研究的非成对多针迹刺绣生成算法基于生成对抗网络，因此先简单介绍了生成对抗网络基本原理和网络特点。最后介绍了目前流行的几种生成对抗网络通用框架，同时分析框架的特点，作为本文网络框架设计的理论支撑。武汉纺织大学硕士学位论文163基于残差注意力的刺绣图像生成本章研究探索基于残差注意力的刺绣图像生成算法，需求是对生成的刺绣图像的色彩和纹理进行增强，使色彩尽可能与参考图保持一致，同时纹理更接近真实刺绣纹理。首先，介绍了刺绣图像生成算法的研究动机。其次，介绍了刺绣图像生成算法的相关基础，包括残差模块和残差注意力网络。最后，介绍了提出的非成对刺绣图像生成算法的网络结构、损失函数以及算法流程。3.1引言与研究动机本文中，AttnEmbGAN的灵感来自于AttentionGAN[9]。AttentionGAN通过使用注意力区分图像的前景和背景来改变图像中的目标。AttnEmbGAN遵循类似的理念，通过注意力来区分色彩、纹理和结构。值得注意的是，AttnEmbGAN仅使用单网络架构，而不是AttentionGAN中的循环网络架构，这大幅降低了训练成本和网络的大小，同时可以产生更好的结果。AttentionGAN使用的一个普通马-斑马数据集。马域到斑马域的转换具有明显的变化特征。因此，网络中的注意力掩码可以使用无监督方法来区分输入图像的前景和背景。然而，当AttentionGAN在刺绣数据集上训练时，很难像从普通马到斑马图像转换的任务中那样，从刺绣图案中区分出前景和背景。图3.1AttnEmbGAN和AttentionGAN的比较Figure3.1AttnEmbGANvsAttentionGAN如图3.1所示，AttentionGAN的注意力模块失效，生成的前景图和背景图完全混杂，无法正常工作。相比之下，AttnEmbGAN中的刺绣残差注意力模块可以从刺绣图中分离出其色彩信息和纹理信息。同时，AttnEmbGAN架构仅使用了AtoB的过程，相比3基于残差注意力的刺绣图像生成17AttentionGAN中AtoBtoA的cycle架构更加轻量。3.2算法相关的前置工作3.2.1残差模块“残差”一词来源于数理统计，用来估算实际观测值与估计值之间的差异（误差观测值）。例如，要找到一个并使方程()=成立，先设定的估计值0，那么残差是−(0)，(−0)是估计误差。图3.2残差模块Figure3.2Residualmodule如图3.2所示，残差块能够越过多层网络传递特征，从而对不同层的特征进行连接。这可以将低层网络中提取的特征直接传递到高层网络，保留和增强输入图的特征。残差块被广泛应用，可以实现特征融合，防止深度网络的过度拟合和退化。在图像生成算法的领域，需要保留输入图的一些特定的特征，残差块能够很好的满足这一需求，使得输出图的一些需要保留的特征不会丢失。残差块在本文算法的网络结构中被广泛使用。3.2.2残差注意力网络残差注意力网络基于软注意力掩码机制，受到残差网络思想的启发，不仅使用注意力掩码处理当前网络层特征，同时也保留上一层网络的特征，从而避免了当网络层数过深时，输入图信息量丢失过多的问题。残差注意力网络包含多层注意力模块，并且将注意力模块分为掩码分支（maskbranch）和主干分支（trunkbranch）。主干分支加工输入图的特征，定义输入图为，其输出为,()；掩码分支生成掩码,()，,()相当于,()的权重。使用类似于残差学习的方式，将得到的注意力特征图与主干特征图进行叠加，输出表示为：,=1+,∗,#式(3.1)通过这种方式，防止掩码分支输出的特征图破坏主干分支的一些需要保留的特征。对应在刺绣生成过程中，结果图需要在添加纹理信息（掩码分支）的同时，保留输入图武汉纺织大学硕士学位论文18的色彩信息（主干分支）。3.3基于残差注意力的刺绣图像生成算法本节介绍本章提出的基于残差注意力的非成对刺绣图像生成算法AttnEmbGAN的网络结构，损失函数，算法流程。在介绍之前，先假设解耦出颜色和纹理表示的刺绣图像属于刺绣图像域。将输入图像转换为刺绣图像的纹理图像和色彩图像的过程描述为映射函数。该函数将内容域映射到刺绣域ℰ，并且使用训练数据()={|i=1…nx}⊂和(ℰ)={|=1…}⊂ℰ，其中nx和分别是参考图像和刺绣图像的总数。3.3.1网络结构为了避免网络生成的注意力掩码和内容掩码的参数相互干扰，类似于相关工作[9]，如图3.3所示，生成器被解耦为共享编码器（灰色）、注意力生成器（橙色）和内容生成器（黄色）。图3.3AttnEmbGAN网络框架Figure3.3OverviewoftheAttnEmbGANarchitecture值得注意的是，注意力掩码被分为纹理注意力掩码、色彩注意力掩码和源注意力掩码，这些注意力掩码对应于刺绣图像的表示。此外，内容掩码被分为纹理内容掩码、色彩内容掩码。为了在彩色图像和纹理图像之间产生显著的差异性，进一步引入了颜色损失。它主要约束生成的彩色图像和输入图像的颜色特征。受到残差学习的启发，网络生成的掩码作为偏置项与输入图像进行融合，得到刺绣彩色图像 和刺绣纹理图像 。（1）生成器框架AttnEmbGAN中的生成器被用来生成注意力掩码:{}=1和内容掩码:{}=1−1，其中是注意力掩码的总数。生成器由三个子网络组成：以提取输入图像的内容特征为目标的共享编码器；注意力生成器用于生成多个注意力掩码；内容生成器用于生成多个内容掩码：3基于残差注意力的刺绣图像生成19=,=#式3.2其中∼为输入图像。（2）刺绣残差注意力模块刺绣残差注意力模块（EmbroideryResidualAttention，ERA）将注意力掩码:{}=1分成刺绣图像中对应的三种表示：纹理注意力掩码:{}=1、色彩注意力掩码:{}=+1−1、原始注意力掩码:{}。此外，内容掩码:{}=1−1被划分为纹理内容掩码:{}=1、色彩内容掩码:{}=+1−1，其中是注意力掩码的总数，是纹理注意力掩码的总数。在实验中，数值被设置为=10,=3。将三组注意力掩码{,,}与对应的两组内容掩码{,}和输入图像相乘，得到刺绣纹理图像 和刺绣色彩图像 ： ==1∗  ==+1−1∗ +∗#式3.3其中∗代表矩阵全元素相乘（哈达马积）。因此，刺绣残差注意力模块最终生成的刺绣结果图 ： ==1∗ +=+1−1∗ +∗#式3.4（3）网络结构相关参数表3.1AttnEmbGAN网络结构相关参数Table3.1ParametersoftheAttnEmbGAN结构层输入通道输出通道大小/步长/填充编码器conv13647×7/1/0conv2641283×3/2/1conv31282563×3/2/1残差块resnet_block1…resnet_block92562563×3/1/1内容生成器conv_transpose12561283×3/2/1,输出填充=1conv_transpose2128643×3/2/1,输出填充=1conv64277×7/1/0注意力生成器conv_transpose12561283×3/2/1,输出填充=1conv_transpose2128643×3/2/1,输出填充=1conv64101×1/1/0本章中AttnEmbGAN网络结构的参数细节见表3.3。值得注意的是，其中每层之间会进行一次实例规范化AdaIN[77]。武汉纺织大学硕士学位论文203.3.2损失函数总损失函数ℒ,由三部分组成：色彩一致性损失函数ℒ，其在生成刺绣色彩图像 的过程中保留了输入图像的颜色特征；对抗性损失函数ℒ,，其鼓励生成的刺绣图像 接近真实刺绣图像；身份一致性损失函数ℒ，可更加稳定的生成刺绣图像 。最终，总损失函数为：ℒ,=ℒ,+ℒ+ℒ#式3.5其中和是用于平衡损失函数的权重。调大的会使生成的刺绣图像 保留更多的输入图像的色彩信息，即色彩会更接近输入图像。在下述的实验中，权重值设置为=5,=0.5，这使得刺绣结果图的纹理和色彩特征得到了很好的平衡。（1）色彩一致性损失函数确保生成的刺绣图像 保留输入图像的色彩信息是刺绣生成中的关键点，即生成的刺绣图像 的色彩需要更接近输入图像的色彩。在AttnEmbGAN中，中间过程中生成的刺绣色彩图像 确定最终生成的刺绣图像的颜色信息。不同于其他通用的风格迁移框架中的内容损失函数，AttnEmbGAN约束中间过程中生成的刺绣色彩图像 。色彩一致性损失函数ℒ鼓励输入图像的色彩特征接近色彩图像 的色彩特征：ℒ=∼−∗+∗1#式3.6其中，是色彩注意力掩码，是源注意力掩码，是色彩内容掩码。∗代表矩阵全元素相乘（哈达马积）。（2）对抗性损失函数在AttnEmbGAN中，利用对抗性损失ℒ,来优化生成器和判别器。由生成器生成的刺绣图像 需要尽可能的接近真实的刺绣图像。同时，判别器需要区分刺绣图是属于生成器生成的刺绣图像 还是真实的刺绣图像。对抗性损失函数ℒ,鼓励生成器生成的刺绣图像 的纹理特征接近真实刺绣图像的纹理特征：ℒ,=∼log+∼log1−#式3.7其中对于对抗性损失函数ℒ，生成器希望减小该值，而判别器希望增大该值。（3）身份一致性损失函数与相关工作[9]不同的是，在刺绣生成任务中，身份一致性损失函数用于保持刺绣纹理生成的稳定性。身份一致性损失函数ℒ定义为：ℒ=∼ℰ−1#式3.83.3.3算法流程基于残差注意力的非成对刺绣图像生成算法AttnEmbGAN的算法流程伪代码如算法3.1所示。在每次迭代开始，对刺绣图像数据进行采样，获得参考图和刺绣图。之3基于残差注意力的刺绣图像生成21后对参考图像进行编码，分别解码获得注意力掩码和内容掩码。使用刺绣残差模块对掩码进行划分和融合，获得刺绣纹理图 和刺绣色彩图 ，最终融合获得刺绣结果图 。计算对抗损失、色彩一致性损失、身份一致性损失，计算总损失函数，反向优化模型的网络参数。算法3.1：AttnEmbGAN算法流程伪代码输入：训练参考图像()和训练刺绣图像(ℰ)；超参数,；迭代次数输出：刺绣图像 ；刺绣纹理图像 ；刺绣色彩图像 模型初始化：生成器（由共享编码器、注意力生成器、内容生成器组成）判别器1:while>do2:从()，(ℰ)中采样获得参考图和刺绣图3://注意力掩码和内容掩码的生成4:获得中间潜码←5:获得注意力掩码:{}=110←6:获得内容掩码:{}=197://获得刺绣纹理图，刺绣色彩图，刺绣结果图8:切分注意力掩码{:{}=13,:{}=49,:{10}}←:{}=1109:切分内容掩码{:{}=13,:{}=49}←:{}=1910:获得刺绣纹理图 ←=13∗ 11:获得刺绣色彩图 ←=49∗ +α10∗12:获得刺绣结果图 ← + 13://反向优化网络14:计算损失函数ℒ←ℒ+ℒ+ℒ15:优化生成器和判别器的网络参数16:=−117:endwhile3.4本章小结本章提出基于残差注意力的非成对刺绣图像生成算法AttnEmbGAN（UnpairedEmbroiderySynthesisviaEmbroideryResidualAttention）。介绍了现有基于生成对抗网络的方法在刺绣图像生成存在的问题，并且受到相关工作的启发，引入残差注意力网络进行刺绣图像的生成。之后介绍了与算法有关的知识基础。最后，本章提出和介绍基于残差注意力的刺绣图像生成算法AttnEmbGAN。武汉纺织大学硕士学位论文224基于多针迹模块的刺绣图像生成本章研究探索非成对条件下的多针迹刺绣图像生成算法AttnMSEmbGAN，目标是在原有模型AttnEmbGAN增强了生成的刺绣图像的色彩一致性和纹理真实性的基础上，对刺绣针迹类型的多样性进一步进行增强，使刺绣针迹风格更加明晰和多样，更接近真实刺绣的纹理特征。首先，介绍了多针迹刺绣图像生成算法的研究动机。其次，介绍了刺绣图像生成算法的前置工作，包括边缘填充操作和三种针迹的区域形状特征。最后，介绍了提出的非成对多针迹刺绣图像生成算法的网络结构、损失函数以及算法流程。4.1引言与研究动机真实的刺绣艺术，会使用多种针迹捕捉参考图的颜色和形状特征。在刺绣领域中，针迹是缝纫过程中单次穿插或移动产生的线环或纱环，这决定了刺绣图像的质量和复杂性。经过调查发现，艺术家需要根据区域形状的一些特征来设计针迹的布局，而不仅仅是捕捉原始图像的形状和颜色信息。一种针法对应一种刺绣纹理风格。同时，一幅完整的刺绣图像通常包含多种针迹类型。在刺绣图像中，可能刺绣纹理风格由于它们对应的针迹类型的不同而不同。因此，高质量的刺绣图像需要有多种针法类型的纹理。换句话说，给定参考图像作为输入，根据该区域的形状特征，每个色彩区域应该填充有相应针迹型的刺绣纹理。4.2算法相关的前置工作4.2.1边缘填充操作图4.1边缘填充Figure4.1SamePadding填充（Padding）是一种常见的图像处理操作，本文中主要使用的是相同元素填充（SamePadding）。如图4.1所示，相同元素填充操作是使用单一元素（橙色）填充图片（蓝色）的边缘，填充的边缘宽度被定义为,图中示例=1。在计算机视觉领域中，填充操作往往用于数据增强，目的是增加数据量、丰富数据多样性、提高模型的泛化能力。本文中为了通过填充无关边缘，增强注意力模块的效果，让网络能够更加稳定的生成刺绣纹理。4基于多针迹模块的刺绣图像生成234.2.2各针迹区域的形状特征在专业的刺绣设计中，针法有几十种。一些针脚类型非常相似，这使得模型学习所有类型的针脚图案显然是不现实的。本文提倡使用以下三种针迹作为基础针迹：缎纹针迹是刺绣中常用的针迹之一。绣针落在物体轮廓两侧，折线呈蛇形前进。普通平针针迹的针孔在轮廓两侧，中间没有针尖。而缎纹针迹的角度因区域变化而变化，密度和线段长度也不同。有一些区域形状较窄且具有一定宽度，使得不可能仅用单一针来填充。因此选择用缎纹针迹填充这些区域，使用多条平行线垂直区域边缘进行填充。榻榻米针迹也叫席文针迹，其纹理与榻榻米相似。在大多数情况下，当使用缎纹针迹填充一大片区域时，只有平行的填充线会使得纹理不够生动。而且据调查，在实际的专业刺绣制版过程中，如果在太宽的区域上强行使用缎针，就会出现跳针的情况。本文提倡使用榻榻米针迹进行填充，这样可以均匀分布针点。平针针迹是一种用刺绣针连续缝制并缝制成线形图案的针迹。它沿着线性图案中的图案路径缝制，表现出不同厚度和形状的线性图案。其中针迹是单线的，所以形状较细。适合在窄区域或参考图的边缘进行填充。平针仅用一根针线填充，包括特殊针迹，如三针。4.3基于多针迹模块的刺绣图像生成算法本节介绍本章提出的基于残差注意力的非成对多针迹刺绣图像生成算法AttnMSEmbGAN的网络结构，损失函数，算法流程。类似AttnEmbGAN，假设解耦出颜色和纹理表示的刺绣图像属于刺绣图像域。将输入图像转换为刺绣图像的纹理图像和色彩图像的过程描述为映射函数。但不同的是，在训练中使用的三种单一针迹、一种混合针迹的刺绣图，针迹类型定义为∈{,,,}，其中:平针针迹、：缎纹针迹、：榻榻米针迹、：混合针迹。该函数将内容域映射到刺绣域ℰ，并且使用训练数据()={|∈{,,,},=1…}⊂和(ℰ)={|∈{,,,},=1…}⊂ℰ，其中nx和分别是参考图像和刺绣图像的总数。4.3.1网络结构为了避免网络生成的注意力掩码和内容掩码的参数相互干扰，类似于AttnEmbGAN，如图3.3所示，生成器被解耦为共享编码器(灰色)、注意力生成器(橙色)和内容生成器(黄色)。与AttnEmbGAN不同的是，在AttnMSEmbGAN中使用了四个判别器，分别是平针针迹判别器、缎纹针迹判别器、榻榻米针迹判别器和混合针迹判别器。（如图4.2所示）值得注意的是，注意力掩码被分为纹理注意力掩码、色彩注意力掩码和源注武汉纺织大学硕士学位论文24意力掩码，这些注意力掩码对应于刺绣图像的三种表示。同时，其中的纹理注意力掩码再次分为三种基础针迹的注意力掩码：平针针迹注意力掩码|；缎纹针迹注意力掩码|；榻榻米针迹注意力掩码|。此外，内容掩码被分为纹理内容掩码、色彩内容掩码。纹理内容掩码也被分为三种基础针迹的内容掩码：平针针迹内容掩码|；缎纹针迹内容掩码|；榻榻米针迹内容掩码|。图4.2AttnMSEmbGAN网络框架Figure4.2OverviewoftheAttnMSEmbGANarchitecture在AttnEmbGAN的基础上，为了在彩色图像和纹理图像之间产生显著的差异性，进一步引入了颜色损失。它主要约束生成的彩色图像和输入图像的颜色特征。受到残差学习的启发，网络生成的掩码作为偏置项与输入图像进行融合，得到刺绣彩色图像 和刺绣纹理图像 。（1）生成器框架AttnMSEmbGAN中的生成器被用来生成注意力掩码:{}=116和内容掩码:{}=115。生成器由三个子网络组成：以提取输入图像的内容特征为目标的共享编码器；注意力4基于多针迹模块的刺绣图像生成25生成器用于生成多个注意力掩码；内容生成器用于生成多个内容掩码：=,=#式4.1其中∼为输入图像。（2）多针迹刺绣残差注意力模块多针迹刺绣残差注意力模块（Multi-stitchEmbroideryResidualAttention，MSERA）将注意力掩码:{}=1分成刺绣图像中对应的三种表示（在实验中，数值被设置为=16,=9）：纹理注意力掩码:{}=19、色彩注意力掩码:{}=1015、原始注意力掩码:{16}。此外，内容掩码:{}=115被划分为纹理内容掩码:{}=19、色彩内容掩码:{}=1015。为了增强生成的刺绣纹理的多针迹特征，纹理注意力掩码:{}=19进一步分为三种基础针迹的注意力掩码：平针针迹注意力掩码|:{}=13；缎纹针迹注意力掩码|:{}=46；榻榻米针迹注意力掩码|:{}=79。纹理内容掩码:{}=19进一步分为三种基础针迹的内容掩码：平针针迹内容掩码|:{}=13；缎纹针迹内容掩码|:{}=46；榻榻米针迹内容掩码|:{}=79。将五组注意力掩码{|,|,|,,}与对应的四组内容掩码{|,|,|,}和输入图像相乘，得到刺绣纹理图像 |和刺绣色彩图像 （针迹类型∈{,,,}）： |==13∗  |==46∗  |==79∗  = |+ |+ | ==1015∗ +16∗#式4.2其中∗代表矩阵全元素相乘（哈达马积）。因此，刺绣残差注意力模块最终生成的刺绣结果图 ： ==13∗ +=46∗ +=79∗ +=1015∗ +∗#式4.3（3）网络结构相关参数本章中AttnMSEmbGAN网络结构的参数细节见表4.1。值得注意的是，其中每层之间会进行一次实例规范化AdaIN[77]。与第三章中的网络AttnEmbGAN不同的是，在AttnMSEmbGAN中，内容生成器的通道数增加至45，注意力生成器的通道数增加至16，这使得纹理掩码拥有足够的通道数分成三种针迹的纹理掩码。武汉纺织大学硕士学位论文26表4.1AttnMSEmbGAN网络结构相关参数Table4.1ParametersoftheAttnMSEmbGAN结构层输入通道输出通道大小/步长/填充编码器conv13647×7/1/0conv2641283×3/2/1conv31282563×3/2/1残差块resnet_block1…resnet_block92562563×3/1/1内容生成器conv_transpose12561283×3/2/1,输出填充=1conv_transpose2128643×3/2/1,输出填充=1conv64457×7/1/0注意力生成器conv_transpose12561283×3/2/1,输出填充=1conv_transpose2128643×3/2/1,输出填充=1conv64161×1/1/0（4）白色填充技巧在实验中，观察到在某些情况下，刺绣纹理不能稳定地生成。因此，文本提出了一种白色填充处理技巧用以解决此问题。图4.3白色填充技巧Figure4.3Thewhitepaddingprocessingtrick如图4.3所示，在网络的预测阶段，将输入图像传入网络之前，首先在输入图像周围填充（padding）一个白色区域。填充的宽度被定义为，在实验中，数值设置为128像素。并且在预测完成之后，对网络生成的输出刺绣图像的非目标区域（即白色区域）进行裁剪，将图片尺寸进行还原。这一技巧的目的是让处理后的输入图像有两个区域:目标区域和非目标区域，使得刺绣通道注意力模块可以更好的聚焦在目标区域进行纹理生成。最后，网络在全局刺绣纹理生成中的稳定性显著提高。4.3.2损失函数总损失函数ℒ,由三部分组成：色彩一致性损失函数ℒ，其在生成刺绣色彩图像 的过程中保留了输入图像的颜色特征；对抗性损失函数ℒ,，其鼓励生成的刺绣图像 接近真实刺绣图像；身份一致性损失函数ℒ，可更加稳定的4基于多针迹模块的刺绣图像生成27生成刺绣图像 。最终，总损失函数为：ℒ|,=ℒ|,+ℒ+|ℒ#式4.4其中∈{,,,}为刺绣针迹的四种基本类型。和是用于平衡损失函数的权重。调大的会使生成的刺绣图像 保留更多的输入图像的色彩信息，即色彩会更接近输入图像。在下述的实验中，权重值设置为=5,=0.5，这使得刺绣结果图的纹理和色彩特征得到了很好的平衡。在实际的训练过程中，每次迭代会传入不同针迹的刺绣图，分别进行训练。在一个周期中，四种针迹的训练次数分别为,平针针迹:缎纹针迹:榻榻米针迹:混合针迹＝{1:1:1:2}。（1）色彩一致性损失函数确保生成的刺绣图像 保留输入图像的色彩信息是刺绣生成中的关键点，即生成的刺绣图像 的色彩需要更接近输入图像的色彩。在AttnMSEmbGAN中，中间过程中生成的刺绣色彩图像 确定最终生成的刺绣图像的颜色信息。不同于其他通用的风格迁移框架中的内容损失函数，AttnMSEmbGAN约束中间过程中生成的刺绣色彩图像 。色彩一致性损失函数ℒ鼓励输入图像的色彩特征接近色彩图像 的色彩特征：ℒ=∼−∗+∗1#式4.5其中，是色彩注意力掩码，是源注意力掩码，是色彩内容掩码。∗代表矩阵全元素相乘（哈达马积）。（2）对抗性损失函数在AttnMSEmbGAN中，利用对抗性损失ℒ|,来优化生成器和判别器，其中∈{,,,}为刺绣针迹的四种基本类型。由生成器生成的刺绣图像 需要尽可能的接近真实的刺绣图像。同时，判别器需要区分刺绣图是属于生成器生成的刺绣图像 还是真实的刺绣图像。对抗性损失函数ℒ,鼓励生成器生成的刺绣图像 的纹理特征接近真实刺绣图像的纹理特征：ℒ|,=∼log+∼log1−#式4.6其中生成器试图对抗性损失函数ℒ|，而判别器试图将其最大化。∈{,,,}为刺绣针迹的四种基本类型。（3）身份一致性损失函数与相关工作[9]不同的是，在刺绣生成任务中，身份一致性损失函数用于保持刺绣纹理生成的稳定性，即保证网络中纹理生成部分能够稳定工作。身份一致性损失函数ℒ定义为：ℒ|=∼ℰ−1#式4.7其中∈{,,,}为刺绣针迹的四种基本类型。武汉纺织大学硕士学位论文284.3.3算法流程基于残差注意力的非成对刺绣图像生成算法AttnMSEmbGAN的算法流程伪代码如算法4.1所示。在每次迭代开始，对刺绣图像数据进行采样，获得参考图和刺绣图。之后对参考图像进行编码，分别解码获得注意力掩码和内容掩码。使用刺绣残差模块对掩码进行划分和融合，获得刺绣纹理图 和刺绣色彩图 ，最终融合获得刺绣结果图 。计算对抗损失、色彩一致性损失、身份一致性损失，计算总损失函数，反向优化模型的网络参数。特别地，在网络的预测阶段，会将针迹类型掩码设置为常量[111]。算法4.1：AttnMSEmbGAN算法流程伪代码输入：训练参考图像()和训练刺绣图像(ℰ)；超参数,；迭代次数；平针针迹训练次数；缎纹针迹训练次数；榻榻米针迹训练次数；混合针迹训练次数输出：刺绣图像 ；刺绣纹理图像 ；刺绣色彩图像 模型初始化：生成器（由共享编码器、注意力生成器、内容生成器组成）判别器（平针针迹判别器、缎纹针迹判别器、榻榻米针迹判别器、混合针迹判别器）1:0=02:while0<do3://获得本次迭代的针迹类型4:if0mod(+++)≤then5:获得针迹类型掩码←[100]//平针针迹的针迹类型掩码6:elseif0mod(+++)≤(+)then7:获得针迹类型掩码←[010]//缎纹针迹的针迹类型掩码8:elseif0mod(+++)≤(++)then9:获得针迹类型掩码←[001]//榻榻米针迹的针迹类型掩码10:else11:获得针迹类型掩码←[111]//混合针迹的针迹类型掩码12:endif13:从()，(ℰ)中采样获得针迹类型为的参考图和刺绣图14://注意力掩码和内容掩码的生成15:获得中间潜码←16:获得注意力掩码:{}=116←17:获得内容掩码:{}=11518://获得刺绣纹理图，刺绣色彩图，刺绣结果图19:切分注意力掩码{|:{}=13,|:{}=46,|:{}=79,:{}=1015,:{16}}←:{}=11620:切分内容掩码{|:{}=13,|:{}=46,|:{}=79,:{}=1015}←:{}=11521:获得针迹类型为的刺绣纹理图 ←∙=13∗ =46∗ =79∗ 4基于多针迹模块的刺绣图像生成2922:获得刺绣色彩图 ←=1015∗ +16∗23:获得刺绣结果图 ← + 24://反向优化网络25:计算损失函数ℒ|←ℒ|+ℒ+ℒ|26:优化生成器和针迹类型为的判别器的网络参数27:0=0+128:endwhile4.4本章小结本章提出基于残差注意力网络的非成对多针迹刺绣图像生成算法AttnMSEmbGAN（UnpairedMulti-stitchEmbroiderySynthesisviaEmbroideryResidualAttention）。首先，介绍了第三章中基于残差注意力网络的非成对刺绣图像生成算法AttnEmbGAN在刺绣针迹类型的多样性上的局限性，对网络框架进行优化，增强了生成刺绣图中针迹类型的特征。之后介绍了该算法相关的前置工作，包括边缘填充操作，各针迹区域的形状特征等。之后，本章提出在非成对条件下基于残差注意力的多针迹刺绣图像生成算法AttnMSEmbGAN并且详细介绍了网络结构，损失函数和算法流程。武汉纺织大学硕士学位论文305实验设计与结果分析本文在第3章提出了AttnEmbGAN算法，在第4章提出了AttnMSEmbGAN算法，本章介绍一些常见的度量方法并且对结果进行分析。首先，介绍了AttnEmbGAN和AttnMSEmbGAN算法的实验设置。其次，介绍了评估图像生成效果的定性和定量方法。最后，对对比实验和消融实验结果进行分析。5.1实验设置5.1.1数据集预处理多针迹刺绣数据集包含超过30K的高质量刺绣图像，包括超过13K对称的参考-刺绣图像和超过17K的非成对图像。它包括三种单针迹和混合了三种单针迹的多针迹。所有训练图像的分辨率会切剪为256×256。在AttnEmbGAN中，训练数据集包含参考图像和刺绣图像，而测试数据集仅包括参考图像。但由于具有完整的卷积网络，测试阶段AttnEmbGAN可以输入任意分辨率的图像。在AttnMSEmbGAN中，网络在三种单针迹数据集上训练生成器中的三种纹理掩码生成器，并且在多针迹数据集上训练色彩掩码生成器。由于AttnMSEmbGAN需要捕捉每种针迹的纹理样式，因此需要单针迹的图像数据。在训练过程中，AttnMSEmbGAN只需要一个非成对的刺绣数据集，这极大的扩展了训练数据的规模。5.1.2实现细节本文中的AttnEmbGAN算法和AttnMSEmbGAN算法使用PyTorch[78]实现。这些实验是在配备V100图形处理器的计算机上进行的。本文实验使用的优化器为Adam（1=0.5,2=0.999）。训练过程中，传入标记有针迹类型的非成对刺绣数据（参考图-刺绣图）。初始化学习率为0.0001,并且在2000次迭代之后进行衰减，实验中批大小（BatchSize）设置为1。其中各项损失函数的权重见下述的超参数实验。5.2评估方法在该章节，介绍了刺绣图像生成结果的定性方法和定量方法，用以评价刺绣图像生成的纹理质量、色彩信息的保持以及针迹类型风格的增强。5.2.1定性评估方法定性评估方法是依靠人眼对样本进行视觉评价，也是评价生成对抗网络的最传统最直观的方法。虽然有着一定的局限性，会受到评论者的主观影响，但是其依旧能很好地5实验设计与结果分析31帮助研究者比较和改进网络模型。在刺绣生成图像中，定性评估方法设计为以下几种：刺绣纹理的质量，生成刺绣纹理的稳定性，刺绣图色彩与参考图色彩的一致性。在用户研究（UserStudy）中，设计了下述三种评价标准：刺绣质量：用户需要评估结果是否近似真实的刺绣，是否有高频噪音或其他伪影，以及针迹类型的多样性。色彩质量：用户需要评估刺绣结果的色彩与输入图像的色彩是否相似，以及是否发生了颜色偏移。纹理质量：用户需要评估刺绣结果的纹理是否具有真实刺绣纹理的特征，以及在某些区域是否无法有效稳定地生成刺绣纹理。5.2.2定量评估方法本文实验使用的定量评估方法包括LPIPS、FID。（1）LPIPSLPIPS（LearnedPerceptualImagePatchSimilarity）[79]是一种基于人类感知相似性度量的图像相似度评价方法。该度量标准会需要一个预训练的生成图像到目标图像的反向网络，并优先处理它们之间的感知相似度。LPIPS比传统方法更类似于人类的视觉感知。LPIPS的值越低，表示生成图像与目标图像越相似，生成的图像质量越好。公式如下：,0=1ℎ,⊙ ℎ− 0ℎ22  #式5.1其中0和分别为目标真实图像和生成图像。为0和之间的距离。从层提取特征堆并在通道维度中进行归一化，利用向量∈ℝ放缩激活通道数,最终计算L2距离。最终在空间上求均值，在通道上求和值。（2）FIDFID（FréchetInceptionDistance）[80]评估两组图像信息的相似度，可通过生成结果和目标结果的距离对生成图像的质量进行评估。计算过程中使用了特征提取网络，该网络会在最后一层输出图像的类别，不过在计算FID的过程中会去除最后的全连接或者池化层。使用计算的生成图像和参考图像高斯分布的Fréchet距离，从而获得两组图像的相似性。显然，两个分布越接近，意味着网络生成的结果图越真实。进一步来说，FID度量的值越小，表明生成结果越近似于目标，也表明生成的图像质量越好。公式如下：,=−22++−20.5#式5.2其中和分别为真实图像和生成图像的提取特征的均值，和分别为真实图像和生武汉纺织大学硕士学位论文32成图像的提取特征的协方差矩阵。5.3对比实验5.3.1超参数调优实验本文提出了刺绣残差注意力模块，用来融合刺绣色彩图像和刺绣纹理图像。经过调优实验，可以先确定身份一致性损失函数的权重=0.5。同时，经过实验可以发现，能够通过调整色彩一致性损失函数的权重来增强刺绣颜色特征或刺绣纹理特征，从而平衡刺绣图像中的颜色信息和纹理信息。图5.1超参数调优实验Figure5.1Hyperparametertuningexperiment如图5.1所示。从左到右，=10,5,2,1,0.5，随着色彩一致性损失函数权重的增加，生成的刺绣图像保留了更多的输入图像的色彩信息。这是因为它增强了颜色细节，并引导刺绣残差注意力更多地关注输入图像的色彩信息。反之，生成的纹理越清晰，细节越多，色彩表示的权重越低，结果图像的色彩越偏离输入图像的色彩，但是结果图像的纹理越丰富，越突出。从实验结果可以发现，AttnEmbGAN生成刺绣的色彩和纹理特征是可控的，并且可以很容易地进行调整。5.3.2定性对比实验本节分别展示了定性对比实验中的实验可视化结果和用户研究评估的结果。（1）对比实验结果在对比实验中，将AttnEmbGAN生成的刺绣图像与最近提出的风格转移方法进行比较:Pix2Pix[23]，CycleGAN[25]，Pixel2Style2Pixel[76]，MUNIT[5]，DRIT[32]，AttentionGAN[9]。我们使用与AttnEmbGAN,AttnMSEmbGAN相同的训练数据集来训练这些模型。特别地，对于Pixel2Style2Pixel，先在(ℰ)的刺绣训练数据集上预先训练了StyleGANv2网络，之后在{(),(ℰ)}的训练数据集上训练了Pixel2Style2Pixel网络，在测试阶段执5实验设计与结果分析33行了风格混合，输入图像潜码的掩码层数为（1-7）,随机采样向量的掩码层数为（8-18）。如图5.2所示，将本文算法与Pix2Pix、CycleGAN、P2S2P、MUNIT、DRIT和AttentionGAN等其他算法做定性比较，结果表明现有算法不能很好地处理刺绣风格。为了显示本文的注意力引导框架的优越性，AttnEmbGAN采用了最新的刺绣生成方法。相比之下，通过使用刺绣通道注意力区分来自刺绣的纹理和颜色表示，本文的AttnEmbGAN，AttnMSEmbGAN模型生成了高质量的结果，并且AttnMSEmbGAN拥有更丰富的针迹风格。图5.2对比实验结果Figure5.2TheresultsofcomparativeexperimentPix2Pix[23]的结果不能很好地捕捉刺绣风格。虽然逐像素损失有助于更好地保存内容，但它们似乎忽略了刺绣图像的纹理。CycleGAN[25]的结果丢失了很多颜色信息。在实验过程中，增加了能够保留输入图像内容信息的身份一致性损失的权重。但是，仍然会发生色偏。Pixel2Style2Pixel[76]的结果不够理想，完全丢失了输入图像的色彩信息。Pixel2Style2Pixel是一种基于StyleGAN的图像到图像的翻译方法。刺绣风格更多的体现在刺绣的纹理层面，而StyleGAN提取风格更多的是语义层面（如脸和猫）。因此，即使实验过程中已经编辑了精细级别的特征，内容信息也不能被有效地呈现。MUNIT[5]的结果不能很好地捕捉输入图像的内容信息和色彩信息。实验表明，在刺绣生成中，内容表示包含部分色彩信息和纹理信息，风格表示也包含这些。因此，生成的图像会丢失输入图像的部分色彩信息，其刺绣纹理不够真实。DRIT[32]的结果单独保留了轻微的刺绣纹理，但颜色特征完全丢失。原始刺绣图像的纹理和色彩风格明显，但由于特征之间的干扰，常见的端到端网络无法很好地保留原始的色彩特征，导致纹理杂乱和颜色不一致。而且训练难度大，经常导致崩溃。值得注武汉纺织大学硕士学位论文34意的是，由于DRIT缺少纹理特征处理模块，生成的刺绣图像风格是随机的。根据输入刺绣图像的纹理风格，不能准确反映不同针法的纹理。AttentionGAN[9]的结果在色彩上与输入图像略有不同，其局部刺绣纹理已经消失。AttentionGAN的注意力机制主要集中在图像的前景和背景上。但刺绣的特性决定了它更适合区分为色彩表示和纹理表示。因此，在这种方法中，前景注意力和背景注意力混合了一些色彩信息和纹理信息，导致其结果的色彩和纹理均不理想。对于刺绣生成，一个好的方法应该是生成出的图像既能保留输入图像的色彩特征，又能呈现出真实刺绣的纹理特征。AttentionGAN将注意力掩码分为前景注意力掩码和背景注意力掩码，用以学习输入图像中的局部变化，但这不能很好地解决从全局内容图像映射到全局刺绣图像的问题。此外，每个生成的刺绣图像不仅需要保留输入图像的色彩信息，还需要在其上自然地添加清晰的刺绣纹理。本文中的AttnEmbGAN方法得益于具有颜色注意力掩码和纹理注意力掩码的结构，并且生成具有高质量细节的刺绣图像。因此，AttnEmbGAN可以生成与输入图像色彩基本相同且纹理更加多样的刺绣结果，其纹理接近真实刺绣的纹理。同时，AttnMSEmbGAN是在AttnEmbGAN的基础上的进一步优化，将纹理注意力掩码进一步划分成三种针迹的纹理注意力掩码，同时添加了对应的判别器和损失函数，极大地增强了刺绣图纹理中的针迹风格。（2）用户研究评估由于刺绣图像的质量在很大程度上受到个人主观内容的影响，因此对刺绣图像的视觉质量进行评价通常具有挑战性。本文进行了用户研究（UserStudy），以显示用户如何评价本文的方法和以前的方法。实验过程中，准备了10幅图像，每幅图像都使用本文的方法和现有方法进行了处理。同时，为刺绣图像设计了三个标准，每位参与者需要对每个图像进行评分，评分范围为1-5分。表5.1用户研究评估结果（平均值/标准差）Table5.1Evaluationresultsofuserstudy方法刺绣质量色彩质量纹理质量Pix2Pix2.510/0.8742.564/0.8412.473/0.877CycleGAN2.917/0.6012.981/0.6242.921/0.619P2S2P1.987/0.6102.020/0.5811.997/0.596MUNIT3.025/0.5692.964/0.5883.018/0.574DRIT2.833/0.6093.112/0.5522.231/0.563AttentionGAN3.987/0.5574.017/0.5753.984/0.557AttnEmbGAN4.042/0.5784.050/0.5824.009/0.561AttnMSEmbGAN4.281/0.5064.053/0.5044.107/0.499如表5.1所示，实验中一共收集了6000个分数，并计算每种方法分数的平均值和标准差。从实验结果可以看出，本文的方法在刺绣质量、颜色质量和纹理质量方面均优于5实验设计与结果分析35现有的方法。由于刺绣残差注意力网络，AttnEmbGAN可以分别生成刺绣图像的颜色图和纹理图，使得生成的刺绣质量高且稳定，其刺绣质量得分高于其他方法。同时本文提出的色彩一致性损失，使刺绣色彩图像接近输入图像的色彩，使其色彩质量得分高于其他方法。同时，在保持颜色稳定的情况下，使用的对抗损失函数和身份一致性损失函数，可以使得刺绣的纹理图像更好地生成，这使得本文算法的纹理质量得分高于其他方法。同时，AttnMSEmbGAN还极大的增强了刺绣纹理中针迹风格的多样性，这使得其刺绣质量和纹理质量的分数进一步的提高。5.3.3定量对比实验表5.2定量对比实验Table5.2Quantitativecomparisonexperiment方法LPIPS（均值/标准差）FIDPix2Pix0.432/0.020239.89CycleGAN0.296/0.016153.49P2S2P0.498/0.007351.70MUNIT0.332/0.018146.14DRIT0.388/0.011174.43AttentionGAN0.296/0.015145.22AttnEmbGAN0.278/0.014123.35AttnMSEmbGAN0.266/0.011121.57如表5.2所示，本文在多针迹刺绣数据集进行刺绣图像生成的定量对比实验。LPIPS基于人类感知相似性去评估度生成结果和目标结果的图像相似度。FID指标评估生成结果和目标结果的特征向量之间的距离。这两种度量均是分值越低，说明生成的结果更近似于目标，视觉效果越好。通过实验结果可以看出，本文中的两种算法均优于目前已有的所有方法。5.4消融实验本文从三个方面进行消融研究:多针迹模块；色彩一致性损失函数ℒ和身份一致性损失函数ℒ；白色填充技巧。5.4.1多针迹模块的消融实验在多针迹模块的消融实验中，研究多针迹模块的作用，其决定了生成结果图中针迹的多样性。在实验过程中，消融了AttnMSEmbGAN中的残差注意力网络的多针迹特性，将三种针迹的纹理注意力机制进行合并，使其退化成AttnEmbGAN。多针迹模块生成包含三种针迹的纹理注意力掩码，最终生成三种针迹的刺绣纹理图。如图5.3所示，在残差注意力网络失去了多针迹特性之后，生成的结果纹理风格趋于一致，部分区域失去了缎纹针迹的风格，也失去了一部分纹理生成的稳定性。消融研究的结果充分展示了多针武汉纺织大学硕士学位论文36迹模块在增强刺绣中针迹风格的多样性。图5.3多针迹模块消融实验Figure5.3AblationstudyontheMulti-stitchembroiderymodule5.4.2相关损失函数的消融实验在损失函数消融实验中，研究色彩一致性损失函数ℒ和身份一致性损失函数ℒ的作用。图5.4损失函数消融实验Figure5.4Ablationstudyonthelossfunction首先，研究色彩一致性损失函数ℒ的作用，它同时也决定了刺绣残差注意力模块的效果。如图5.4所示，在没有色彩一致性损失的情况下，生成结果不能保留输入图像的色彩信息，并且其纹理是不规则和无序的。因此，我们的颜色损失可以有效地控制和5实验设计与结果分析37保存来自输入图像的色彩特征。受相关工作[25]的启发，AttnEmbGAN中也使用了身份一致性损失函数ℒ。本文进行消融实验来研究身份一致性损失函数ℒ在刺绣生成工作中的作用。如图5.4所示，其增强刺绣纹理特征。在没有身份一致性损失的情况下，网络不能在结果图像的一些局部区域中正确地生成刺绣纹理。因此，身份一致性损失可以显著提高局部刺绣纹理生成的稳定性。消融研究的定量评估结果（FID和LPIPS评分）如表5.3所示。表5.3损失函数消融研究的定量实验Table5.2Quantitativeexperimentofablationstudyonthelossfunction方法LPIPS（均值/标准差）FIDw/oℒ0.371/0.019165.80w/oℒ0.279/0.015126.80AttnEmbGAN0.278/0.014123.355.4.3白色填充技巧的消融实验为了稳定地生成全局刺绣纹理，在预测阶段，我们提出了一种称为白色填充的处理技巧。图5.5白色填充技巧消融实验Figure5.5Ablationstudyonthewhitepaddingprocessingtrick如图5.5所示，在没有白色填充（即设置=0）的情况下，在该示例中，网络几乎不能正确地生成刺绣纹理。这主要是因为在目标区域（输入图像）周围填充白色区域可以更好地引导刺绣通道注意力集中在目标区域上。因此，白色填充处理技巧可以显著提高全局刺绣纹理生成的稳定性。5.5本章小结本章是对本文中提出的AttnEmbGAN算法和AttnMSEmbGAN算法的实验结果进行武汉纺织大学硕士学位论文38展示和分析。首先，介绍了AttnEmbGAN和AttnMSEmbGAN算法实验的实验设置（数据集预处理和实现细节）。介绍了评估图像生成效果的定性方法和定量方法。最后展示和分析了实验结果。通过实验，本章说明了本文提出的AttnEmbGAN算法在非成对条件下比现有的图像生成算法生成的刺绣图像质量更高，尽可能的保留了参考图的色彩信息，同时刺绣纹理更接近真实刺绣。同时，本文提出的AttnMSEmbGAN算法在非成对条件下生成的多针迹刺绣图像，在保有AttnEmbGAN所有优点的基础上，对生成刺绣的针迹类型的多样性进行了进一步的增强。6总结以及展望396总结以及展望6.1总结刺绣作为一种传统的艺术形式，因具备鲜明的色彩风格和独特的针织质感的纹理特征，一直以来广受用户的喜爱和关注。然而，请艺术家手工制作一幅刺绣图像是一项费力且需要大量手工技能的工作，因此自动刺绣图像生成是一项具有挑战性和价值的任务。目前已有的图像到图像转换的方法，生成的刺绣图像失去了大量的参考图像的色彩信息，这导致会发生颜色偏移的问题。其次，复杂的刺绣纹理使得这些方法生成的刺绣纹理杂乱甚至消失。最后，生成具有针迹特征鲜明的刺绣虽然出现在传统方法，但对于现有的基于学习的方法十分困难。为了有效地解决上述问题，受到相关工作的启发，本文提出了在非成对条件下，基于残差注意力网络的刺绣图像生成算法AttnEmbGAN（UnpairedEmbroiderySynthesisviaEmbroideryResidualAttention）。该算法通过一个刺绣残差注意力模块（EmbroideryResidualAttention，ERA），对刺绣图像中的色彩表示和纹理表示进行解耦。之后通过色彩一致性损失、对抗损失、身份一致性损失对刺绣的色彩和纹理进行约束，使得生成的刺绣结果图尽可能的保留了参考图像的色彩信息，同时纹理更接近真实刺绣。在与现有流行的几种算法的对比实验中，通过定性比较和定量比较，充分展示该算法在刺绣图像生成上的优越性。并且对该算法中的创新点进行了消融实验，充分展示了改进的有效性。一幅真实的刺绣，往往会包含多种针迹类型。为了增强生成的刺绣图的针迹类型的多样性，本文基于AttnEmbGAN进行了进一步的改进，提出了在非成对条件下，基于残差注意力网络的多针迹刺绣图像生成算法AttnMSEmbGAN（UnpairedMulti-stitchEmbroiderySynthesisviaEmbroideryResidualAttention）。该算法对AttnEmbGAN中的纹理生成部分进行了优化，从生成一张纹理图进化为生成三张基础针迹的纹理图。并且对色彩一致性损失、对抗损失、身份一致性损失进行了针对性的改进，使其更好的适配新的网络框架。与此同时，为了解决偶尔出现的刺绣纹理生成失败的问题，本文还提出了白色填充技巧，进一步提升了刺绣纹理生成的稳定性。在与现有流行的几种算法和AttnEmbGAN的对比实验中，通过定性比较和定量比较，充分展示该算法在刺绣图像生成上的优越性。并且对该算法中的创新点进行了消融实验，充分展示了改进的有效性，极大地增强了生成刺绣针迹类型的多样性。现有的提供给深度神经网络学习的刺绣图像数据集非常稀少，且质量和规模不够理想。本文制作和公开了一个全新的多针迹刺绣数据集。该数据集是首个对刺绣图像标记了三种基础针迹类型的数据集。其在规模上也远超之前的刺绣数据集。武汉纺织大学硕士学位论文406.2展望本文提出的算法的结果已经达到预期的效果，但仍然存在进一步改进的空间。在技术层面，生成的刺绣图像的视觉效果可做出进一步的提高。最近一段时间，随着生成式人工智能的发展，使用稳定扩散模型[81]进行图像生成的工作逐渐流行，生成的图像效果十分理想。但因该模型提出的时间与本文撰写时间过于接近，暂时没有充足的时间对其在刺绣生成领域上做出尝试。用其替代掉原有的基于残差块的生成网络，或许能够进一步优化结果。同时，目前基于神经渲染（NeRF）[82]的三维生成也十分流行，三维刺绣图的生成也是一个值得尝试的方向。在实现层面，数据集还能进行进一步的扩充。同时，模型的训练过程中或许可以增加新的约束，用以进一步增强图像的某些特征。参考文献41参考文献[1]GuanX,LuoL,LiH,etal.Automaticembroiderytexturesynthesisforgarmentdesignandonlinedisplay[J].TheVisualComputer,2021,37:2553-2565.[2]LiuY,WrightJ,AlvaradoA.MakingBeautifulEmbroideryfor“Frozen2”[C].SpecialInterestGrouponComputerGraphicsandInteractiveTechniquesConferenceTalks.2020:1-2.[3]BegMA,YuJY.Generatingembroiderypatternsusingimage-to-imagetranslation[J].arXivpreprintarXiv:2003.02909,2020.[4]WeiZ,KoYC.Segmentationandsynthesisofembroideryartimagesbasedondeeplearningconvolutionalneuralnetworks[J].InternationalJournalofPatternRecognitionandArtificialIntelligence,2022,36(11):2252018.[5]HuangX,LiuMY,BelongieS,etal.Multimodalunsupervisedimage-to-imagetranslation[C].ProceedingsoftheEuropeanconferenceoncomputervision(ECCV).2018:172-189.[6]KimT,ChaM,KimH,etal.Learningtodiscovercross-domainrelationswithgenerativeadversarialnetworks[C].Internationalconferenceonmachinelearning.PMLR,2017:1857-1865.[7]LiuMY,BreuelT,KautzJ.Unsupervisedimage-to-imagetranslationnetworks[J].Advancesinneuralinformationprocessingsystems,2017:1–9.[8]RichardsonE,AlalufY,PatashnikO,etal.Encodinginstyle:astyleganencoderforimage-to-imagetranslation[C].ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.2021:2287-2296.[9]TangH,LiuH,XuD,etal.Attentiongan:Unpairedimage-to-imagetranslationusingattention-guidedgenerativeadversarialnetworks[J].IEEEtransactionsonneuralnetworksandlearningsystems,2021:1–16.[10]ChenX,McCoolM,KitamotoA,etal.Embroiderymodelingandrendering[M].ProceedingsofGraphicsInterface2012.2012:131-139.[11]CuiD,ShengY,ZhangG.Image‐basedembroiderymodelingandrendering[J].ComputerAnimationandVirtualWorlds,2017,28(2):e1725.1–12.[12]ShenQ,CuiD,ShengY,etal.Illumination-preservingembroiderysimulationfornon-photorealisticrendering[C].MultiMediaModeling:23rdInternationalConference,MMM2017,Reykjavik,Iceland,January4-6,2017,Proceedings,PartII23.SpringerInternationalPublishing,2017:233-244.[13]YangK,ZhouJ,SunZ,etal.Image-basedirregularneedlingembroideryrendering[C].proceedingsofthe5thinternationalsymposiumonvisualinformationcommunicationandinteraction.2012:87-94.[14]ZhouJ,SunZ,YangK.Acontrollablestitchlayoutstrategyforrandomneedle武汉纺织大学硕士学位论文42embroidery[J].JournalofZhejiangUniversitySCIENCEC,2014,15(9):729-743.[15]YangK,SunZ,MaC,etal.Paintwithstitches:Arandom-needleembroideryrenderingmethod[M].Proceedingsofthe33rdcomputergraphicsInternational.2016:9-12.[16]YangK,SunZ.Paintwithstitches:astyledefinitionandimage-basedrenderingmethodforrandom-needleembroidery[J].MultimediaToolsandApplications,2018,77:12259-12292.[17]QianW,XuD,CaoJ,etal.Aestheticartsimulationforembroiderystyle[J].MultimediaToolsandApplications,2019,78:995-1016.[18]MaC,SunZ.StitchGeneration:Modelingandcreationofrandom-needleembroiderybasedonMarkovchainmodel[J].MultimediaToolsandApplications,2019,78:34065-34094.[19]TakahashiY,FukusatoT.Stitch:Aninteractivedesignsystemforhand-sewnembroidery[M].ACMSIGGRAPH2018Posters.2018:1-2.[20]QianW,CaoJ,XuD,etal.Cnn-basedembroiderystylerendering[J].InternationalJournalofPatternRecognitionandArtificialIntelligence,2020,34(14):2059045.[21]GoodfellowI,Pouget-AbadieJ,MirzaM,etal.Generativeadversarialnetworks[J].CommunicationsoftheACM,2020,63(11):139-144.[22]DumoulinV,BelghaziI,PooleB,etal.Adversariallylearnedinference[J].arXivpreprintarXiv:1606.00704,2016:1–18.[23]IsolaP,ZhuJY,ZhouT,etal.Image-to-imagetranslationwithconditionaladversarialnetworks[C].ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.2017:1125-1134.[24]MirzaM,OsinderoS.Conditionalgenerativeadversarialnets[J].arXivpreprintarXiv:1411.1784,2014:1–7.[25]ZhuJY,ParkT,IsolaP,etal.Unpairedimage-to-imagetranslationusingcycle-consistentadversarialnetworks[C].ProceedingsoftheIEEEinternationalconferenceoncomputervision.2017:2223-2232.[26]KarrasT,AittalaM,LaineS,etal.Alias-FreeGenerativeAdversarialNetworks[J].AdvancesinNeuralInformationProcessingSystems,2021,34:852-863.[27]KarrasT,LaineS,AittalaM,etal.Analyzingandimprovingtheimagequalityofstylegan[C].ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.2020:8110-8119.[28]KingmaDP,WellingM.Auto-EncodingVariationalBayes[J].InternationalConferenceonLearningRepresentations,2014,pp.1–14.[29]LarsenABL,SønderbySK,LarochelleH,etal.Autoencodingbeyondpixelsusingalearnedsimilaritymetric[C].Internationalconferenceonmachinelearning.PMLR,2016:1558-1566.[30]SohnK,LeeH,YanX.Learningstructuredoutputrepresentationusingdeepconditional参考文献43generativemodels[J].Advancesinneuralinformationprocessingsystems,2015:3483–3491.[31]BaoJ,ChenD,WenF,etal.CVAE-GAN:fine-grainedimagegenerationthroughasymmetrictraining[C].ProceedingsoftheIEEEinternationalconferenceoncomputervision.2017:2745-2754.[32]LeeHY,TsengHY,MaoQ,etal.Drit++:Diverseimage-to-imagetranslationviadisentangledrepresentations[J].InternationalJournalofComputerVision,2020,128:2402-2417.[33]XuD,WangW,TangH,etal.Structuredattentionguidedconvolutionalneuralfieldsformonoculardepthestimation[C].ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.2018:3917-3925.[34]LiangX,ZhangH,XingEP.Generativesemanticmanipulationwithcontrastinggan[J].ProceedingsoftheEuropeanconferenceoncomputervision(ECCV),2018.[35]SunR,HuangC,ZhuH,etal.Mask-awarephotorealisticfacialattributemanipulation[J].ComputationalVisualMedia,2021,7(3):363-374.[36]MoS,ChoM,ShinJ.Instagan:Instance-awareimage-to-imagetranslation[J].ICLR,2019.[37]ChenX,XuC,YangX,etal.Attention-ganforobjecttransfigurationinwildimages[C].ProceedingsoftheEuropeanconferenceoncomputervision(ECCV).2018:164-180.[38]KastaniotisD,NtinouI,TsourounisD,etal.Attention-awaregenerativeadversarialnetworks(ATA-GANs)[C].2018IEEE13thImage,Video,andMultidimensionalSignalProcessingWorkshop(IVMSP).IEEE,2018:1-5.[39]YangC,KimT,WangR,etal.Show,attend,andtranslate:Unsupervisedimagetranslationwithself-regularizationandattention[J].IEEETransactionsonImageProcessing,2019,28(10):4845-4856.[40]ZhangH,GoodfellowI,MetaxasD,etal.Self-attentiongenerativeadversarialnetworks[C].Internationalconferenceonmachinelearning.PMLR,2019:7354-7363.[41]KimJ,KimM,KangH,etal.U-GAT-IT:UnsupervisedGenerativeAttentionalNetworkswithAdaptiveLayer-InstanceNormalizationforImage-to-ImageTranslation[C].InternationalConferenceonLearningRepresentations,2020.[42]AlamiMejjatiY,RichardtC,TompkinJ,etal.Unsupervisedattention-guidedimage-to-imagetranslation[J].Advancesinneuralinformationprocessingsystems,2018,31.[43]YiZ,ZhangH,TanP,etal.Dualgan:Unsupervisedduallearningforimage-to-imagetranslation[C].ProceedingsoftheIEEEinternationalconferenceoncomputervision.2017:2849-2857.[44]ChoiY,ChoiM,KimM,etal.Stargan:Unifiedgenerativeadversarialnetworksformulti-domainimage-to-imagetranslation[C].ProceedingsoftheIEEEconferenceon武汉纺织大学硕士学位论文44computervisionandpatternrecognition.2018:8789-8797.[45]TangH,XuD,WangW,etal.Dualgeneratorgenerativeadversarialnetworksformulti-domainimage-to-imagetranslation[C].ComputerVision–ACCV2018:14thAsianConferenceonComputerVision,Perth,Australia,December2–6,2018,RevisedSelectedPapers,PartI.Cham:SpringerInternationalPublishing,2019:3-21.[46]LiB,QiX,LukasiewiczT,etal.Controllabletext-to-imagegeneration[J].AdvancesinNeuralInformationProcessingSystems,2019,32.[47]YuX,ChenY,LiuS,etal.Multi-mappingimage-to-imagetranslationvialearningdisentanglement[J].AdvancesinNeuralInformationProcessingSystems,2019,32.[48]MaL,JiaX,SunQ,etal.Poseguidedpersonimagegeneration[J].Advancesinneuralinformationprocessingsystems,2017,30.[49]TangH,WangW,XuD,etal.Gestureganforhandgesture-to-gesturetranslationinthewild[C].Proceedingsofthe26thACMinternationalconferenceonMultimedia.2018:774-782.[50]DongH,LiangX,GongK,etal.Soft-gatedwarping-ganforpose-guidedpersonimagesynthesis[J].Advancesinneuralinformationprocessingsystems,2018,31.[51]WangTC,LiuMY,TaoA,etal.Few-shotvideo-to-videosynthesis[J].Advancesinneuralinformationprocessingsystems,2019.[52]TangH,XuD,SebeN,etal.Multi-channelattentionselectionganwithcascadedsemanticguidanceforcross-viewimagetranslation[C].ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.2019:2417-2426.[53]RegmiK,BorjiA.Cross-viewimagesynthesisusingconditionalgans[C].ProceedingsoftheIEEEconferenceonComputerVisionandPatternRecognition.2018:3501-3510.[54]TangH,XuD,YanY,etal.Localclass-specificandglobalimage-levelgenerativeadversarialnetworksforsemantic-guidedscenegeneration[C].ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.2020:7870-7879.[55]AlBaharB,HuangJB.Guidedimage-to-imagetranslationwithbi-directionalfeaturetransformation[C].ProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision.2019:9016-9025.[56]WangM,YangGY,LiR,etal.Example-guidedstyle-consistentimagesynthesisfromsemanticlabeling[C].ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.2019:1495-1504.[57]XuD,WangW,TangH,etal.Structuredattentionguidedconvolutionalneuralfieldsformonoculardepthestimation[C].ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.2018:3917-3925.[58]VaswaniA,ShazeerN,ParmarN,etal.Attentionisallyouneed[J].Advancesinneuralinformationprocessingsystems,2017,30.[59]金志凌，朱鸿雨，苏玉兰，等.基于多粒度交互推理的答案选择方法研究[J].中文参考文献45信息学报,2023,37(1):104-111,120.[60]ZhangH,GoodfellowI,MetaxasD,etal.Self-attentiongenerativeadversarialnetworks[C].Internationalconferenceonmachinelearning.PMLR,2019:7354-7363.[61]YeJ,JiY,SongJ,etal.TowardsEnd-to-EndEmbroideryStyleGeneration:APairedDatasetandBenchmark[C].PatternRecognitionandComputerVision:4thChineseConference,PRCV2021,Beijing,China,October29–November1,2021,Proceedings,PartIV4.SpringerInternationalPublishing,2021:201-213.[62]GoldbergBJ,TsonisA.Methodformodifyingembroiderydesignprograms:U.S.Patent5,270,939[P].1993-12-14.[63]KyprianidisJE,CollomosseJ,WangT,etal.Stateofthe"art”:Ataxonomyofartisticstylizationtechniquesforimagesandvideo[J].IEEEtransactionsonvisualizationandcomputergraphics,2012,19(5):866-885.[64]YiR,XiaM,LiuYJ,etal.LinedrawingsforfaceportraitsfromphotosusingglobalandlocalstructurebasedGANs[J].IEEETransactionsonPatternAnalysisandMachineIntelligence,2020,43(10):3462-3475.[65]ShuY,YiR,XiaM,etal.Gan-basedmulti-stylephotocartoonization[J].IEEETransactionsonVisualizationandComputerGraphics,2021,28(10):3376-3390.[66]ShiY,DebD,JainAK.Warpgan:Automaticcaricaturegeneration[C].ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.2019:10762-10771.[67]TianY,RenJ,ChaiM,etal.Agoodimagegeneratoriswhatyouneedforhigh-resolutionvideosynthesis[J].arXivpreprintarXiv:2104.15069,2021.[68]XiongW,LuoW,MaL,etal.Learningtogeneratetime-lapsevideosusingmulti-stagedynamicgenerativeadversarialnetworks[C].ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.2018:2364-2373.[69]SaitoM,MatsumotoE,SaitoS.Temporalgenerativeadversarialnetswithsingularvalueclipping[C].ProceedingsoftheIEEEinternationalconferenceoncomputervision.2017:2830-2839.[70]YuL,ZhangW,WangJ,etal.Seqgan:Sequencegenerativeadversarialnetswithpolicygradient[C].ProceedingsoftheAAAIconferenceonartificialintelligence.2017,31(1).[71]LinK,LiD,HeX,etal.Adversarialrankingforlanguagegeneration[J].Advancesinneuralinformationprocessingsystems,2017:3155-3165.[72]GuoJ,LuS,CaiH,etal.Longtextgenerationviaadversarialtrainingwithleakedinformation[C].ProceedingsoftheAAAIconferenceonartificialintelligence.2018,32(1).[73]JhamtaniH,Berg-KirkpatrickT.Modelingself-repetitioninmusicgenerationusingstructuredadversaries[C].Proc.MachineLearningforMediaDiscoveryWorkshop,extendedabstract.2019.[74]LiuHM,YangYH.Leadsheetgenerationandarrangementbyconditionalgenerative武汉纺织大学硕士学位论文46adversarialnetwork[C].201817thIEEEInternationalConferenceonMachineLearningandApplications(ICMLA).IEEE,2018:722-727.[75]CífkaO,ŞimşekliU,RichardG.SupervisedSymbolicMusicStyleTranslationUsingSyntheticData[C].20thInternationalSocietyforMusicInformationRetrievalConference(ISMIR).2019.[76]RichardsonE,AlalufY,PatashnikO,etal.Encodinginstyle:astyleganencoderforimage-to-imagetranslation[C].ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.2021:2287-2296.[77]HuangX,BelongieS.Arbitrarystyletransferinreal-timewithadaptiveinstancenormalization[C].ProceedingsoftheIEEEinternationalconferenceoncomputervision.2017:1501-1510.[78]PaszkeA,GrossS,ChintalaS,etal.Automaticdifferentiationinpytorch[J].NeurIPSWorkshop,pp.1–4,2017.[79]ZhangR,IsolaP,EfrosAA,etal.Theunreasonableeffectivenessofdeepfeaturesasaperceptualmetric[C].ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.2018:586-595.[80]DowsonDC,LandauBV.TheFréchetdistancebetweenmultivariatenormaldistributions[J].Journalofmultivariateanalysis,1982,12(3):450-455.[81]RombachR,BlattmannA,LorenzD,etal.High-resolutionimagesynthesiswithlatentdiffusionmodels[C].ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2022:10684-10695.[82]MildenhallB,SrinivasanPP,TancikM,etal.Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis[J].CommunicationsoftheACM,2021,65(1):99-106.

