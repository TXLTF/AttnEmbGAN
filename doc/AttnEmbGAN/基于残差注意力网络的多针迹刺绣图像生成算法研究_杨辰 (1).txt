分类号 TP391.4 学校代码10495
ＵＤＣ 004.8 密级公开
硕士学位论文
基于残差注意力网络的
多针迹刺绣图像生成算法研究
作者姓名： 杨辰
学号： 2015363088
指导教师： 胡新荣教授
学科门类： 工程
专业： 软件工程
研究方向： 计算机视觉
完成日期： 二零二三年六月

WuhanTextileUniversity
M.E.Dissertation
Multi-StitchEmbroideryImageGeneration
viaResidualAttentionNetwork
Candidate： YangChen
Supervisor： Prof.HuXinrong
Time: June2023

摘要
刺绣作为一个古老的艺术形式，其相关的图像生成工作一直广受工业界和学界的关
注。现实中的刺绣，往往会具有鲜亮的色彩、复杂的纹理和种类多样的针迹。这使得刺
绣图像生成是一个具有挑战性的课题。然而，使用现有的传统方法和基于生成对抗网络
的方法所生成的刺绣图案，会出现色偏、纹理杂乱以及失去原有结构等问题。同时，其
中基于生成对抗网络的方法并没有考虑刺绣针迹对刺绣的影响。围绕上述问题，本文对
多针迹刺绣图案生成任务展开研究：
（1）本文提出了一种基于残差注意力网络的刺绣图像生成网络框架。受到残差注
意力网络的启发，针对刺绣生成任务设计了三种注意力掩码，分别是色彩注意力掩码、
纹理注意力掩码、源注意力掩码。在非成对数据集条件下，该网络能够生成刺绣的色彩
和纹理并进行融合，避免刺绣结果图出现色偏、纹理杂乱甚至失去输入图的结构等问题。
（2）同时，本文对上述网络做出进一步优化，提出了基于残差注意力网络的多针
迹刺绣图像生成网络框架。通过添加一个多针迹模块，大幅度增强了结果图中针迹风格
的多样性。提出的多针迹模块可根据色彩区域的形状特征匹配合适的针迹类型，从而生
成具有多种针迹风格的刺绣图，使结果进一步接近真实的刺绣。同时在研究过程中，发
现了一种白色填充技巧，大幅提高预测阶段中网络生成刺绣纹理的稳定性，解决了生成
过程中偶尔出现的无法正常生成纹理的问题。
（3）在研究过程中制作了一个多针迹刺绣数据集。该数据集包含成对的参考图和
刺绣图，每一张刺绣图标记有一种刺绣针迹类型标签。目前该数据集包括榻榻米针迹、
平针针迹、缎纹针迹三种针迹。据调研，该数据集不仅是首个公开的标有针迹类型的刺
绣数据集，同时也是目前规模最大的刺绣图像数据集。
大量的定性实验和定量实验均可表明，本文提出的网络框架生成的刺绣图像，优于
现有的方法。在用户研究和对比实验中，本文刺绣结果的色彩更接近输入图，刺绣纹理
更加逼真，纹理包含了三种针迹风格。本文算法生成的刺绣图，在弗雷歇起始距离（FID）
和可学习感知图像块相似度（LPIPS）两种度量评估的分数均小于其他方法，说明其刺
绣图像分布更接近真实的刺绣图像分布。
关键词：多针迹刺绣生成；风格迁移；生成对抗网络；残差注意力网络
研究类型：应用基础研究

Abstract
Asanancientartform,therelatedworkofembroideryimagesynthesishasbeenwidely
followedbytheacademic.Realembroideryimageoftenboastsvibrantcolors,intricate
textures,anddiversestitchtype.Thismakesembroiderysynthesisachallengingtask.
However,existingtraditionalmethodsandthosebasedonGenerativeAdversarialNetworks
(GANs)forgeneratingembroideryimagessufferfromproblemssuchascolorshift,chaotic
textures,andlossoforiginalstructure.Additionally,previousworksbasedonGANsdidnot
takeintoaccounttheimpactofstitchtypesonembroidery.Therefore,thispapermainly
exploresandstudiesthemulti-stitchembroiderysynthesis:
(1)Thispaperproposesanetworkframeworkforembroideryimagegenerationvia
residualattentionnetwork.Basedontheideaofresidualattention,threeattentionmasksare
designed,namelycolorattentionmask,textureattentionmaskandsourceattentionmask.
Undertheunpaireddatasets,thenetworkcangenerateembroiderycolorandtextureimages
andfusethem.Soastoavoidtheproblemsofcolorcast,messytextureandevenlosingthe
structureoftheinputpictureinthegeneratedembroiderypicture.
(2)Thispaperfurtherupdatetheabovenetworkandproposesanetworkframeworkfor
multi-stitchembroideryimagegenerationviaresidualattentionnetwork.Byaddinga
multi-stitchmodule,thediversityofstitchstylesintheresultsisgreatlyenhanced.The
proposedmulti-stitchmodulecanmatchtheappropriatestitchtypesaccordingtotheshape
characteristicsofcolorregions,makingtheresultshavevariousstitchstylesandclosertothe
realembroidery.Intheresearchprocess,awhitefillingtechniquewasfound,whichgreatly
improvedthestabilityofembroiderytexturegeneratedbynetworkinthepredictionstage.It
solvestheproblemthatthetexturecannotbegeneratednormallyoccasionallyinthe
generationprocess.
(3)Amulti-stitchembroiderydatasetwasproducedinthispaper.Thisdatasetconsistsof
pairsofreferenceimagesandcorrespondingembroideryimages,witheachembroideryimage
beinglabeledwithatypeofembroiderystitchtype.Currently,thedatasetincludesthreestitch
types:tatamistitch,flatstitch,andsatinstitch.Accordingtoresearch,thisdatasetisnotonly
thefirstpubliclyavailableembroiderydatasetthatislabeledwithstitchtypes,butalsothe
largestembroideryimagedatasetcurrentlyavailablefornetworklearning.
Extensivequalitativeandquantitativeexperimentsdemonstratethattheembroidery

imagesgeneratedbytheproposednetworkframeworkinthispaperaresuperiortoexisting
methods.Inuserstudyandcomparativeexperiments,thecolorofresultsisclosertotheinput,
andtheembroiderytextureismorerealistic.Andthetexturecontainsthreestitchstyles.In
quantitativeexperiments,theresultsinthispaperhaslowerscoresinFréchetInception
Distance(FID)andLearnedPerceptualImagePatchSimilarity(LPIPS)thanothermethods,
whichshowsthattheembroideryresultsdistributionisclosertotherealembroideryimage
distribution.
Keywords:Multi-stitchembroiderygeneration;Styletransfer;Generativeadversarial
networks;Residualattentionnetwork
Thesis:Appliedbasicresearch

目  录 
1 绪论  ................................ ................................ ................................ .........  1 
1.1 研究背景及意义  ................................ ................................ ...............  1 
1.2 国内外的研究现状  ................................ ................................ ...........  1 
1.2.1 基于图像的传统生成方法  ................................ .........................  2 
1.2.2 基于深度学习的生成方法  ................................ .........................  2 
1.3 论文的主要工作和研究内容  ................................ ..........................  4 
1.4 论文的组织结构  ................................ ................................ ...............  5 
2 相关知识与理论基础  ................................ ................................ ............  6 
2.1 三种基础刺绣针迹  ................................ ................................ ...........  6 
2.2 刺绣数据集  ................................ ................................ .......................  6 
2.2.1 单针迹刺绣数据集  ................................ ................................ ..... 7 
2.2.2 多针迹刺绣数据集  ................................ ................................ ..... 7 
2.3 基于传统方法的刺绣图像生成  ................................ ......................  8 
2.4 生成对抗网络的原理与特点  ................................ ..........................  9 
2.4.1 基本原理  ................................ ................................ .....................  9 
2.4.2 网络特点  ................................ ................................ ...................  10 
2.5 基于生成对抗网络的图像生成  ................................ ....................  10 
2.5.1 pix2pix  ................................ ................................ ........................  10 
2.5.2 CycleGAN  ................................ ................................ ..................  11 
2.5.3 MUNIT  ................................ ................................ .......................  12 
2.5.4 DRIT  ................................ ................................ ..........................  12 
2.5.5 Pixel2Style2Pixel  ................................ ................................ ....... 13 
2.5.6 AttentionGAN  ................................ ................................ ............  14 
2.6 本章小结  ................................ ................................ .........................  14 
3 基于残差注意力的刺绣图像生成  ................................ ......................  16 
3.1 引言与研究动机  ................................ ................................ .............  16 
3.2 算法相关的前置工作  ................................ ................................ ..... 17 
3.2.1 残差模块  ................................ ................................ ...................  17 
3.2.2 残差注意力网络  ................................ ................................ ....... 17 
3.3 基于残差注意力的刺绣图像生成算法  ................................ ........  18 
3.3.1 网络结构  ................................ ................................ ...................  18 
3.3.2 损失函数  ................................ ................................ ...................  20 
3.3.3 算法流程  ................................ ................................ ...................  20 
3.4 本章小结  ................................ ................................ .........................  21 

4 基于多针迹模块的刺绣图像生成  ................................ ......................  22 
4.1 引言与研究动机  ................................ ................................ .............  22 
4.2 算法相关的前置工作  ................................ ................................ ..... 22 
4.2.1 边缘填充操作  ................................ ................................ ...........  22 
4.2.2 各针迹区域的形状特征  ................................ ...........................  23 
4.3 基于多针迹模块的刺绣图像生成算法  ................................ ........  23 
4.3.1 网络结构  ................................ ................................ ...................  23 
4.3.2 损失函数  ................................ ................................ ...................  26 
4.3.3 算法流程  ................................ ................................ ...................  28 
4.4 本章小结  ................................ ................................ .........................  29 
5 实验设计与结果分析  ................................ ................................ ..........  30 
5.1 实验设置  ................................ ................................ .........................  30 
5.1.1 数据集预处理  ................................ ................................ ...........  30 
5.1.2 实现细节  ................................ ................................ ...................  30 
5.2 评估方法  ................................ ................................ .........................  30 
5.2.1 定性评估方法  ................................ ................................ ...........  30 
5.2.2 定量评估方法  ................................ ................................ ...........  31 
5.3 对比实验  ................................ ................................ .........................  32 
5.3.1 超参数调优实验  ................................ ................................ ....... 32 
5.3.2 定性对比实验  ................................ ................................ ...........  32 
5.3.3 定量对比实验  ................................ ................................ ...........  35 
5.4 消融实验  ................................ ................................ .........................  35 
5.4.1 多针迹模块的消融实验  ................................ ...........................  35 
5.4.2 相关损失函数的消融实验  ................................ .......................  36 
5.4.3 白色填充技巧的消融实验  ................................ .......................  37 
5.5 本章小结  ................................ ................................ .........................  37 
6 总结以及展望  ................................ ................................ ......................  39 
6.1 总结  ................................ ................................ ................................ . 39 
6.2 展望  ................................ ................................ ................................ . 40 
参考文献  ................................ ................................ ................................ ... 41 

1绪论
11绪论
1.1研究背景及意义
刺绣是一种古老的艺术形式，使用针和线在织物上绣制各种装饰性图案，捕捉各种
美丽图案的色彩特征。然而，手动创建刺绣图像是一项费力且需要大量手工技能的工作，
因此自动刺绣图像生成是一项具有挑战性和价值的任务。在设计刺绣时，艺术家需要考
虑参考图像的颜色信息，并缝制合适的刺绣纹理。经过调查研究，高质量的刺绣图像需
要满足三个基本条件：（1）刺绣的颜色应尽可能接近参考图像的颜色；（2）刺绣的纹
理应该是均匀，整洁和多样化的；（3）一幅刺绣往往包含多种针迹。
在刺绣艺术中，利用不同的针迹展现出具有纹理特征和图案的多彩形态的织物结构。
针迹由缝纫针以不同的速度、质量和复杂度在各个层次上编织交叉的线或纱而成。由此
制成的刺绣图案是美丽而复杂的。随着机器学习的发展，从图像中训练计算机识别和再
现刺绣图案变得可能、必要和有用。然而，专业的刺绣中使用了数十种针法，有些针法
非常相似，这使机器学习变得复杂。目前，消费者希望将计算机运用在他们的刺绣图案
定制项目中，并实现个性化的风格。作为纺织品生产的工匠，这些人希望能够自动高效
地依据参考图生成对应刺绣图，以便能够将更多的时间专注于他们的创意作品。
虽然艺术形式的图像风格化已经得到广泛研究，但是刺绣生成的风格化仍然是一个
重要挑战。传统的刺绣生成方法[1,2]使用传统算法渲染刺绣纹理，这使得它耗时且费力，
并且生成的刺绣纹理不够真实。最近，基于学习的刺绣生成方法[3,4]在图像可以基于提供
的输入图像进行风格化方面引起了相当大的关注。
近年来，尽管基于学习的风格化方法取得了显著的成功，包括图像到图像的转换[5-8]
以及注意力指导的图像到图像的转换[9]，但它们通常无法生成令人满意的刺绣图像。造
成这种情况的原因有三个。首先，刺绣图像的颜色需要尽可能接近参考图像的颜色，使
得没有相关约束的方法会导致颜色偏移的问题。其次，刺绣纹理过于复杂和特殊，这使
得这些方法生成的刺绣纹理太杂乱甚至消失。最后，往往一幅刺绣具有多种针迹，但生
成具有针迹特征鲜明的刺绣对于现有基于学习的方法十分困难。
如何有效地解决上述问题，是多针迹刺绣生成的难题。本文旨在研究效果更好的多
针迹刺绣生成算法，实现色彩接近参考图，纹理接近真实刺绣，包含多种针迹的刺绣风
格转换。
1.2国内外的研究现状
近年来，刺绣图像生成在国内外被广泛研究。从早期基于图像的传统方法到如今的
深度学习方法，各个时期都有其大量的研究典例。

武汉纺织大学硕士学位论文
21.2.1基于图像的传统生成方法
在刺绣图像生成的领域，最具有挑战的部分，是需要同时让刺绣的色彩接近参考图，
刺绣的纹理接近真实刺绣。经过训练的艺术家能够快速捕捉参考图的色彩特征，通过比
对找出最接近参考图的色彩。同时，艺术家们依据区域不同的形状特征，设计出多种针
迹，这些针迹决定了刺绣的纹理风格。
将刺绣艺术家这种能力迁移到计算机上，依据输入的参考图渲染出对应的刺绣图，
便是早期的计算机方法——基于图像的传统生成方法。最初的传统方法，仅仅考虑一种
针迹。Chen等人[10]提出了一种从线条图案自动生成刺绣图案的技术。Cui等人[11]将基于
笔画的渲染技术与Phong光照模型相结合，解决了之前Chen等人工作[10]中存在的不连
贯的阴影问题。在刺绣的光照方面，Shen等人[12]提出了一种保持照明的刺绣模拟方法。
而乱针绣是一种更为复杂的风格。Yang等人[13]提出了一种多层渲染技术来生成乱
针绣纹样，而Zhou等人[14]为每个区域添加了新的针迹属性。之后的工作逐渐开始关注
生成图像中的针迹风格。例如，Yang等人[15,16]关注了针迹构造的本质，揭示了与乱针绣
相关的不同艺术风格。Qian等人[17]向灰度图像添加噪声并应用风格转移方法以改善模型
训练。Chen等人[18]提出了一种使用马尔科夫链模型的乱针绣纹样生成器。
尽管这些方法已经有了一定的进展，但它们太过依赖用户设置图像分割数据和针迹
风格参数[19]，使得它们费时且劳动密集。而且，这些方法不够自动化，无法让非领域专
家使用它们来解释和创建刺绣设计。
近年来，一些方法试图解决这些问题。Liu等人[2]提出了一种新的基于曲线的方法
来程序化地生成复杂的刺绣，该方法可以采用由线条笔划表示的2D视觉设计并产生可
渲染的曲线。Guan等人[1]提出了一个基于自动纹理生成的框架，将任意输入图像转换成
刺绣风格的艺术，用于服装设计和在线显示。值得注意的是，这项工作关注了刺绣针迹
对于刺绣纹理风格质量的影响，开始尝试依据色彩区域的形状特征匹配适合的刺绣针迹
类型，生成对应的刺绣纹理。然而，由于传统刺绣生成方法的局限性，使用该方法生成
的刺绣纹理单调，与真实刺绣相差较大。
1.2.2基于深度学习的生成方法
（1）基于深度学习的刺绣生成
近几年，开始出现基于深度学习的刺绣图像生成方法，目前这些方法主要生成单针
迹刺绣。Qian等人[20]提出了一个包含两个损失函数的CNN来生成乱针绣图像，Wei等
人[4]使用语义分割方法从刺绣综合过程中提取目标内容图像。由于上述针迹风格是随机
的，不能用来检查多个针迹构造的组合。也就是说，乱针绣只能被拆解为单针成分风格。
Beg等人[3]提出了一种无监督的图像到图像转换方法来生成刺绣图像，但它也呈现出单
针特征。因此，我们选择关注不同针迹类型的组合对生成刺绣图像的质量和复杂度的影

1绪论
3响。
（2）图像到图像转换的通用网络框架
图像到图像的转换是从某个图像域映射到另一个图像域的工作，现已存在大量的通
用框架。生成对抗网络（GAN）在图像到图像的翻译方面取得了显著进展[21]。在此基础
之上，已经有了大量成熟的工作[22,23]提供了图像到图像翻译问题的解决方案。其中
Pix2Pix[23]是基于条件GAN[24]的通用框架。然而，获得足够的成对刺绣图像数据集具有
挑战性，这些方法需要使用成对的图像数据集进行训练，这是刺绣风格化的困难之处。
为了解决以上问题，CycleGAN[25]是最先提出的非成对的学习两个域之间的图像转
换，通过引入循环一致性损失函数而不需要成对的数据。StyleGAN[26,27]可以在潜在编码
层面上修改生成的面部信息，但它在刺绣生成任务中无法获得良好的结果。UNIT模型[7]
是基于不同数据空间共享相同潜在空间的假设而设计的。因此，可以将图像到图像转换
问题视为潜在空间转换。MUNIT[5]将共享的潜在空间定义为内容空间，差异潜在空间是
用于执行多模态图像到图像转换的样式空间。一些工作[28,29]使潜码服从高斯分布，使得
网络更便于预测潜码信息。之后其他工作[30]增加了基于变分自编码器（VAE）的条件信
息，同时Bao等人[31]在其基础上添加了GAN模块。多样的图像到图像表征分离转换
（DRIT++）[32]也使用VAE来解决分离表示的任务。然而，这些方法不仅无法生成具
有多针迹风格的刺绣图像，并且它们处理图案复杂性的能力较差。换句话说，这些方法
生成的刺绣图色彩和纹理均无法令人满意。色彩上与原图差距较大，也就是提到的出现
色彩偏移的问题。同时，纹理作为刺绣图像的高频特征是本文的关键学习目标，但上述
方法生成的刺绣结果图与真实刺绣图差距较大，而且常常会出现生成纹理失效的问题。
（3）基于注意力机制的图像到图像转换
为了克服这些问题，本研究继而将目标转向基于注意力机制的图像到图像转换的方
法。注意力机制已成功地引入了许多计算机视觉应用，例如深度估计[33]，帮助模型专注
于输入的相关部分以解决相应的输出，而无需任何监督。受此启发，一些工作使用注意
模块以一种无监督的方式关注图像翻译任务的感兴趣区域，可以分为两类。
第一类方法是使用额外的数据提供注意力机制。例如，Liang等人提出了
ContrastGAN[34]，它使用每个数据集的对象掩码注释作为额外的输入数据。Sun等人[35]
使用全卷积网络生成的掩码编辑面部属性。此外，InstaGAN[36]结合实例信息（例如对象
分割掩码）并改进了多实例转换。
第二类是训练另一个分割或关注模型来生成注意力映射并将其拟合到系统中。例如，
Chen等人[37]使用额外的注意力网络生成注意力映射，以便更多关注可以放在感兴趣的
对象上。Kastaniotis等人提出了ATAGAN[38]，它使用知识蒸馏生成注意力映射。Yang
等人[39]建议添加一个注意力模块进行预测。Zhang等人提出了SAGAN[40]用于图像生成
任务。Kim等人[41]建议使用辅助分类器生成注意力蒙版。Mejjati等人[42]提出了与生成

武汉纺织大学硕士学位论文
4器、鉴别器和其他两个注意力网络联合训练的注意力机制。
然而，上述方法都需要使用额外的网络或数据来获得注意力掩码，这增加了整个系
统的参数数量、训练时间和存储空间。为了解决此问题，Tang等人提出了AttentionGAN[9]，
将注意力方法嵌入到原始生成器中，因此不需要任何额外的模型来获取感兴趣物体的注
意力掩码。值得注意的是，该方法也可以用于多域图像到图像转换的任务[43]中。
注意力机制指导的图像到图像转换遵守一些由外部指定的约束条件，例如类别标签
[44,45]，文本描述[46,47]，人体关键点和骨骼[48,50]分割地图[51-54]和参考图像[55,56]。由于不同
的生成任务需要不同的指导信息，现有的工作针对特定应用进行了定制，即使用特别设
计的网络架构和训练目标。例如，Ma等人提出了PG2[48]，它是一个两阶段框架，使用
姿势掩码损失来生成基于人物图像和人体姿势关键点的人物图像。Tang等人提出
GestureGAN[49]，它是一个前向-后向一致性架构，采用所提出的颜色损失来生成基于输
入图像和条件手骨架的新手势图像。Wang等人提出了Vid2Vid框架[51]，它使用一个精
心设计的权重生成模块来生成视频，真实地反映输入图像的风格和条件分割地图的布局。
注意力学习在计算机视觉和自然语言处理中已经得到广泛应用[58,59]。为了提高图像生成
性能，注意力机制最近也在图像到图像转换任务中进行了研究[60]。
1.3论文的主要工作和研究内容
本文主要探索和研究基于残差注意力网络的非成对多针迹刺绣图像生成。
随着深度学习和生成对抗网络的日益发展，本文将多针迹刺绣图像生成过程定义为
从参考图域到刺绣图域的一种映射。
之前从CycleGAN[6]框架延伸出的刺绣图像生成算法[3]，存在刺绣结果图的色彩与参
考图的色彩差距较大，纹理不够真实等问题。而且基于深度学习生成的刺绣图无法像传
统方法那样拥有多种针迹。同时，成对的刺绣图像数据获取难度较大。上述问题使得非
成对的多针迹刺绣图像生成具有很大的挑战性。
受到AttentionGAN[9]的启发，本文提出了基于残差注意力网络的刺绣图像生成算法
AttnEmbGAN（UnpairedEmbroideryGenerationUsingEmbroideryResidualAttention）。
该算法通过残差注意力模块解耦刺绣图像的色彩表示和纹理表示来进行建模，通过色彩
注意力、纹理注意力、源图注意力三种注意力掩码生成刺绣图像。通过大量的定性和定
量实验，充分展示了本文算法的结果明显优于现有的几种主流方法。
为了增强刺绣针迹类型的多样性，本文改进了AttnEmbGAN，提出了基于残差注意
力网络的多针迹刺绣图像生成算法AttnMSEmbGAN（UnpairedMulti-StitchEmbroidery
SynthesisUsingEmbroideryResidualAttention）。该算法将AttnEmbGAN中的纹理注意
力掩码进行了进一步的分割（平针针迹纹理注意力、缎纹针迹纹理注意力和榻榻米针迹
纹理注意力）。并且设计了一个多针迹模块，在网络的预测阶段辅助网络生成具有多种

1绪论
5针迹纹理的刺绣纹理图。定性和定量地对比现流行的几种风格迁移算法和AttnEmbGAN，
该算法不仅保留了原AttnEmbGAN生成的刺绣图像的优点，同时多种针迹纹理生成的
能力得到了大幅增强。
1.4论文的组织结构
本文围绕基于残差注意力网络的多针迹刺绣图像生成算法展开研究。总共分为以下
六章：
第1章，绪论。围绕研究背景，分析了国内外刺绣图像生成的研究现状。介绍工作
内容和组织结构。
第2章，展开介绍了多针迹刺绣图像生成相关的理论基础。首先是一个公开的多针
迹刺绣图像数据集，介绍了其制作流程，这也是此项工作中的一个贡献。然后，介绍了
基于传统方法的刺绣图像生成。同时，由于本文的多针迹刺绣图像生成使用了生成对抗
网络框架，因此需要深入分析生成对抗网络的基础内容。最后介绍了目前流行的几种生
成对抗网络的通用框架。
第3章，提出了基于残差注意力网络的刺绣图像生成算法AttnEmbGAN。先分析了
现有工作的问题，基于刺绣本身的特点，启发设计新的框架对刺绣图像进行针对性的非
成对的刺绣图像生成。然后介绍了新算法的前置知识，包括残差模块、残差注意力网络。
详细介绍了基于残差注意力网络的非成对刺绣图像生成算法AttnEmbGAN的相关内容。
第4章，在AttnEmbGAN的基础上，提出增强了刺绣针迹类型的基于残差注意力
网络的多针迹刺绣图像生成算法AttnMSEmbGAN。先分析了第3章的AttnEmbGAN算
法在生成包含多种针迹纹理刺绣图任务中存在的缺陷。然后介绍了新算法的前置知识，
包括边缘填充操作、各针迹区域的形状特征。之后详细介绍了提出的同样基于残差注意
力网络的多针迹刺绣图像生成算法AttnMSEmbGAN的相关内容及白色填充技巧。
第5章，实验结果的展示和分析。先介绍了AttnEmbGAN和AttnMSEmbGAN算法
实验的数据集预处理和代码实现细节。现有流行的定性和定量的分析方法。最后是实验
结果（定性对比实验、定量对比实验、消融实验）的展示和分析。
第6章，本文的总结和展望。首先对本文进行总结，之后进一步展望未来的研究和
工作方向。

武汉纺织大学硕士学位论文
62相关知识与理论基础
围绕本文提出的基于残差注意力网络的多针迹图像生成算法，本章对其相关知识和
理论基础进行介绍。首先，介绍了三种基础的刺绣针迹。其次，介绍现有的已公开的刺
绣数据集和本工作中制作的全新的多针迹刺绣数据集。之后，介绍了生成对抗网络的相
关基础。最后，展开讨论现流行的几种基于生成对抗网络的图像生成方法。
2.1三种基础刺绣针迹
图2.1三种基础针迹
Figure2.1Thethreebasicstitches
刺绣艺术通常由各种针迹和表现为刺绣图案的颜色形式组成。制作刺绣需要技术和
审美能力。针迹类型直接决定了最终纹理效果的特征。本研究的重点是研究多针迹刺绣
模型和其对应的数据集，因为现有针迹多达几十种，因此在研究中使用三种基础针迹（缎
纹针迹，榻榻米针迹和平针针迹），如图2.1所示。图中小图是针迹的走针线路，大图
是渲染出来的对应的刺绣纹理。这三种针法的特点描述如下：
缎纹针迹是一种非常常见的刺绣针迹。针头沿物体轮廓两侧落下，折叠的线条呈蛇
形移动。缎纹针的角度、密度和长度因物体而异。
榻榻米针迹，也称为席纹针，类似于榻榻米草编织纹路。刺绣针法形成一个块状表
面，遵循线性行进路径，形成统一有序的针群。
平针针迹是另一种常见的刺绣针迹，其中针沿着线性路径连续缝制，形成具有不同
厚度和形状表现的图案。
2.2刺绣数据集
多针迹刺绣图像生成需要大量的刺绣图像数据，并且不需要对每张刺绣图标记对应
的刺绣针迹。首先，不同针迹类型的刺绣图，其纹理风格差别是十分巨大的。其次，刺
绣图像数据获取成本昂贵，现有公开的刺绣数据集很少，且规模较小。并且没有标记针
迹类型的刺绣数据集。因此本研究过程中，制作了首个标记了针迹类型的刺绣数据集—
—多针迹刺绣数据集，同时也是目前规模最大的刺绣数据集（图片总数超过30000张）。

2相关知识与理论基础
72.2.1单针迹刺绣数据集
刺绣图像数据可以分为单针迹图像数据和多针迹刺绣图像数据。这主要取决于刺绣
图像是否标记了对应的针迹类型。
目前现有的刺绣数据集，是一个包含有9000组输入草图和相应的输出刺绣图像的
成对的刺绣数据集[61]，但是这类数据集依然具有其局限性。在传统的刺绣生成方法中，
刺绣的针迹类型逐渐被重视，多针迹刺绣图像的生成已经开始流行。但由于现存的所有
刺绣数据集，均没有标记刺绣的针迹信息。因此，基于深度学习的刺绣生成方法，目前
仅仅只能进行简单的端到端的刺绣风格转换工作，无法用于较为复杂的多针迹刺绣图像
生成。
2.2.2多针迹刺绣数据集
在本文研究中，为社区贡献了一个多针迹刺绣数据集。多针迹绣数据集有一些很好
的属性。首先，这种刺绣的针迹是由经验丰富的刺绣设计师设计的，这意味着每个区域
匹配的针迹类型都是最符合其区域形状特征的。其次，多针迹绣数据集使用了三种基础
针迹类型[62]进行标记，包括缎纹针迹、榻榻米针迹和平针针迹。其三，该数据集规模超
过30000张参考图与刺绣图数据，包含成对的和非成对的两部分。其四，刺绣图案由专
业的刺绣设计软件渲染。最后，数据集中的图像没有光影等冗余信息干扰模型的学习，
同时也更便于后期的二次渲染。
图2.2多针迹刺绣数据集
Figure2.2Multi-stitchEmbroideryDataset
如图2.2所示，多针迹刺绣数据集中的数据被标记为四种标签，分别对应于三种基
础针迹类型（缎纹针迹、榻榻米针迹、平针针迹）和一种混合了三种单针类型的多针迹
类型。数据集包括成对的和非成对的部分，成对的部分包括参考图和对应的刺绣图。
多针迹刺绣数据集的刺绣图案由刺绣设计师设计，并通过专业刺绣软件渲染。每个
区域都与最适合的刺绣类型相匹配。为确保模型能够学习输入图像的相应特征，没有选
择过于复杂的图案。在数据集中，刺绣图案设置为单一的刺绣类型和单一的颜色。缝合
数据的部分应尽可能平均。最终，将获得一组参考/刺绣图像对。以下是刺绣图案制作的

武汉纺织大学硕士学位论文
8步骤：
（1）绘制参考图案
在进行刺绣版制作之前，先需要绘制带有刺绣颜色信息的参考图，作为刺绣图像制
作的模板。数据集中的内容图像应包括形状信息和色彩信息。数据集中的大部分内容图
像都足够简单且色彩清晰。具有简单特征的刺绣图像可以使网络更快地收敛。同时，还
挑选了部分具有复杂形状和颜色的图片，这使得网络可以处理一些复杂的图像。
（2）针迹设计
对于不同区域形状的参考图案，设计师填充一种与之匹配针迹纹理。不同宽度的形
状适合不同的针织样式。对于针织类型的参数设置，我们尽可能保持一致性，以便于提
取纹理特征。
（3）依据针织类型进行多层渲染
使用专业的刺绣版制作软件进行渲染和生成刺绣模拟图像。其分辨率经过调整，能
够清晰地表示针迹的细节。因此，在训练过程中，当网络对数据集进行局部采样时，仍
然可以保留刺绣纹理的细节。此外，当使用基于全卷积的网络进行训练时，也意味着可
以获得其他分辨率的结果。
图2.3多针迹刺绣数据集的分布
Figure2.3DatadistributionoftheMulti-stitchEmbroideryDataset
多针迹刺绣数据集包含超过30000的高质量刺绣图像，包括超过13000的对齐内容
刺绣图像和超过17000的未对齐图像，具体分布如图2.3所示。
2.3基于传统方法的刺绣图像生成
在介绍基于深度学习的方法之前，还需简单了解基于传统方法的刺绣图像生成。
由于现有的刺绣数据集规模较小，针迹类型信息的缺失，这导致基于生成对抗网络
的刺绣图像生成的结果不一定优于最新的传统刺绣图像生成方法[1]。

武汉纺织大学硕士学位论文
14而Pixel2Style2Pixel便是在这个基础上，使用标准特征金字塔提取特征图，传入
StyleGAN中的不同层级（StyleGAN中网络的层级高低决定了其编辑的属性），如图2.8
所示。
Pixel2Style2Pixel将StyleGAN应用到了图像到图像转换的领域。
2.5.6AttentionGAN
目前，有大量工作将注意力机制应用于GAN，其中一些方法需要使用额外的网络
或数据来获得注意力掩码，这将导致训练成本十分昂贵。基于这项问题，AttentionGAN[9]
被提出，将注意力机制嵌入生成器中，分别生成注意力掩码和内容掩码，不需要额外的
任何模型。
图2.9AttentionGAN简介
Figure2.9OverviewofAttentionGAN
如图2.9所示，AttentionGAN由一个编码器和两个生成器组成。两个生成器分别直
接生成内容掩码和注意力掩码，同时将注意力掩码继续分成前景注意力掩码和背景注意
力掩码。之后对图像重新进行融合。整个网络框架同样遵循了CycleGAN提出的“循环
一致性”概念。因此这项工作使用的数据集也是非成对的。
AttentionGAN使用注意力机制区分前景和后景，然而刺绣生成中更注重色彩和纹理，
因此AttentionGAN直接用于刺绣生成的结果并不理想。
2.6本章小结
本章主要阐述非成对多针迹刺绣生成的前置知识以及相关工作。首先介绍了刺绣的
三种针迹，并且举例介绍了现有的刺绣图像数据集，同时介绍了本研究中新制作的数据

2相关知识与理论基础
15集的特点和优势。本研究的网络训练也需要基于该数据集。同时，由于本文研究的非成
对多针迹刺绣生成算法基于生成对抗网络，因此先简单介绍了生成对抗网络基本原理和
网络特点。最后介绍了目前流行的几种生成对抗网络通用框架，同时分析框架的特点，
作为本文网络框架设计的理论支撑。

武汉纺织大学硕士学位论文
163基于残差注意力的刺绣图像生成
本章研究探索基于残差注意力的刺绣图像生成算法，需求是对生成的刺绣图像的色
彩和纹理进行增强，使色彩尽可能与参考图保持一致，同时纹理更接近真实刺绣纹理。
首先，介绍了刺绣图像生成算法的研究动机。其次，介绍了刺绣图像生成算法的相关基
础，包括残差模块和残差注意力网络。最后，介绍了提出的非成对刺绣图像生成算法的
网络结构、损失函数以及算法流程。
3.1引言与研究动机
本文中，AttnEmbGAN的灵感来自于AttentionGAN[9]。AttentionGAN通过使用注意
力区分图像的前景和背景来改变图像中的目标。AttnEmbGAN遵循类似的理念，通过注
意力来区分色彩、纹理和结构。值得注意的是，AttnEmbGAN仅使用单网络架构，而不
是AttentionGAN中的循环网络架构，这大幅降低了训练成本和网络的大小，同时可以
产生更好的结果。
AttentionGAN使用的一个普通马-斑马数据集。马域到斑马域的转换具有明显的变
化特征。因此，网络中的注意力掩码可以使用无监督方法来区分输入图像的前景和背景。
然而，当AttentionGAN在刺绣数据集上训练时，很难像从普通马到斑马图像转换的任
务中那样，从刺绣图案中区分出前景和背景。
图3.1AttnEmbGAN和AttentionGAN的比较
Figure3.1AttnEmbGANvsAttentionGAN
如图3.1所示，AttentionGAN的注意力模块失效，生成的前景图和背景图完全混杂，
无法正常工作。相比之下，AttnEmbGAN中的刺绣残差注意力模块可以从刺绣图中分离
出其色彩信息和纹理信息。同时，AttnEmbGAN架构仅使用了AtoB的过程，相比

武汉纺织大学硕士学位论文
34意的是，由于DRIT缺少纹理特征处理模块，生成的刺绣图像风格是随机的。根据输入
刺绣图像的纹理风格，不能准确反映不同针法的纹理。
AttentionGAN[9]的结果在色彩上与输入图像略有不同，其局部刺绣纹理已经消失。
AttentionGAN的注意力机制主要集中在图像的前景和背景上。但刺绣的特性决定了它更
适合区分为色彩表示和纹理表示。因此，在这种方法中，前景注意力和背景注意力混合
了一些色彩信息和纹理信息，导致其结果的色彩和纹理均不理想。
对于刺绣生成，一个好的方法应该是生成出的图像既能保留输入图像的色彩特征，
又能呈现出真实刺绣的纹理特征。AttentionGAN将注意力掩码分为前景注意力掩码和背
景注意力掩码，用以学习输入图像中的局部变化，但这不能很好地解决从全局内容图像
映射到全局刺绣图像的问题。此外，每个生成的刺绣图像不仅需要保留输入图像的色彩
信息，还需要在其上自然地添加清晰的刺绣纹理。
本文中的AttnEmbGAN方法得益于具有颜色注意力掩码和纹理注意力掩码的结构，
并且生成具有高质量细节的刺绣图像。因此，AttnEmbGAN可以生成与输入图像色彩基
本相同且纹理更加多样的刺绣结果，其纹理接近真实刺绣的纹理。同时，AttnMSEmbGAN
是在AttnEmbGAN的基础上的进一步优化，将纹理注意力掩码进一步划分成三种针迹
的纹理注意力掩码，同时添加了对应的判别器和损失函数，极大地增强了刺绣图纹理中
的针迹风格。
（2）用户研究评估
由于刺绣图像的质量在很大程度上受到个人主观内容的影响，因此对刺绣图像的视
觉质量进行评价通常具有挑战性。本文进行了用户研究（UserStudy），以显示用户如
何评价本文的方法和以前的方法。实验过程中，准备了10幅图像，每幅图像都使用本
文的方法和现有方法进行了处理。同时，为刺绣图像设计了三个标准，每位参与者需要
对每个图像进行评分，评分范围为1-5分。
表5.1用户研究评估结果（平均值/标准差）
Table5.1Evaluationresultsofuserstudy
方法 刺绣质量 色彩质量 纹理质量
Pix2Pix 2.510/0.874 2.564/0.841 2.473/0.877
CycleGAN 2.917/0.601 2.981/0.624 2.921/0.619
P2S2P 1.987/0.610 2.020/0.581 1.997/0.596
MUNIT 3.025/0.569 2.964/0.588 3.018/0.574
DRIT 2.833/0.609 3.112/0.552 2.231/0.563
AttentionGAN 3.987/0.557 4.017/0.575 3.984/0.557
AttnEmbGAN 4.042/0.578 4.050/0.582 4.009/0.561
AttnMSEmbGAN 4.281/0.506 4.053/0.504 4.107/0.499
如表5.1所示，实验中一共收集了6000个分数，并计算每种方法分数的平均值和标
准差。从实验结果可以看出，本文的方法在刺绣质量、颜色质量和纹理质量方面均优于

武汉纺织大学硕士学位论文
38展示和分析。首先，介绍了AttnEmbGAN和AttnMSEmbGAN算法实验的实验设置（数
据集预处理和实现细节）。介绍了评估图像生成效果的定性方法和定量方法。最后展示
和分析了实验结果。通过实验，本章说明了本文提出的AttnEmbGAN算法在非成对条
件下比现有的图像生成算法生成的刺绣图像质量更高，尽可能的保留了参考图的色彩信
息，同时刺绣纹理更接近真实刺绣。同时，本文提出的AttnMSEmbGAN算法在非成对
条件下生成的多针迹刺绣图像，在保有AttnEmbGAN所有优点的基础上，对生成刺绣
的针迹类型的多样性进行了进一步的增强。

6总结以及展望
396总结以及展望
6.1总结
刺绣作为一种传统的艺术形式，因具备鲜明的色彩风格和独特的针织质感的纹理特
征，一直以来广受用户的喜爱和关注。然而，请艺术家手工制作一幅刺绣图像是一项费
力且需要大量手工技能的工作，因此自动刺绣图像生成是一项具有挑战性和价值的任务。
目前已有的图像到图像转换的方法，生成的刺绣图像失去了大量的参考图像的色彩信息，
这导致会发生颜色偏移的问题。其次，复杂的刺绣纹理使得这些方法生成的刺绣纹理杂
乱甚至消失。最后，生成具有针迹特征鲜明的刺绣虽然出现在传统方法，但对于现有的
基于学习的方法十分困难。
为了有效地解决上述问题，受到相关工作的启发，本文提出了在非成对条件下，基
于残差注意力网络的刺绣图像生成算法AttnEmbGAN（UnpairedEmbroiderySynthesisvia
EmbroideryResidualAttention）。该算法通过一个刺绣残差注意力模块（Embroidery
ResidualAttention，ERA），对刺绣图像中的色彩表示和纹理表示进行解耦。之后通过
色彩一致性损失、对抗损失、身份一致性损失对刺绣的色彩和纹理进行约束，使得生成
的刺绣结果图尽可能的保留了参考图像的色彩信息，同时纹理更接近真实刺绣。在与现
有流行的几种算法的对比实验中，通过定性比较和定量比较，充分展示该算法在刺绣图
像生成上的优越性。并且对该算法中的创新点进行了消融实验，充分展示了改进的有效
性。
一幅真实的刺绣，往往会包含多种针迹类型。为了增强生成的刺绣图的针迹类型的
多样性，本文基于AttnEmbGAN进行了进一步的改进，提出了在非成对条件下，基于
残差注意力网络的多针迹刺绣图像生成算法AttnMSEmbGAN（UnpairedMulti-stitch
EmbroiderySynthesisviaEmbroideryResidualAttention）。该算法对AttnEmbGAN中的
纹理生成部分进行了优化，从生成一张纹理图进化为生成三张基础针迹的纹理图。并且
对色彩一致性损失、对抗损失、身份一致性损失进行了针对性的改进，使其更好的适配
新的网络框架。与此同时，为了解决偶尔出现的刺绣纹理生成失败的问题，本文还提出
了白色填充技巧，进一步提升了刺绣纹理生成的稳定性。在与现有流行的几种算法和
AttnEmbGAN的对比实验中，通过定性比较和定量比较，充分展示该算法在刺绣图像生
成上的优越性。并且对该算法中的创新点进行了消融实验，充分展示了改进的有效性，
极大地增强了生成刺绣针迹类型的多样性。
现有的提供给深度神经网络学习的刺绣图像数据集非常稀少，且质量和规模不够理
想。本文制作和公开了一个全新的多针迹刺绣数据集。该数据集是首个对刺绣图像标记
了三种基础针迹类型的数据集。其在规模上也远超之前的刺绣数据集。

武汉纺织大学硕士学位论文
406.2展望
本文提出的算法的结果已经达到预期的效果，但仍然存在进一步改进的空间。在技
术层面，生成的刺绣图像的视觉效果可做出进一步的提高。最近一段时间，随着生成式
人工智能的发展，使用稳定扩散模型[81]进行图像生成的工作逐渐流行，生成的图像效果
十分理想。但因该模型提出的时间与本文撰写时间过于接近，暂时没有充足的时间对其
在刺绣生成领域上做出尝试。用其替代掉原有的基于残差块的生成网络，或许能够进一
步优化结果。同时，目前基于神经渲染（NeRF）[82]的三维生成也十分流行，三维刺绣
图的生成也是一个值得尝试的方向。在实现层面，数据集还能进行进一步的扩充。同时，
模型的训练过程中或许可以增加新的约束，用以进一步增强图像的某些特征。

参考文献
41参考文献
[1]GuanX,LuoL,LiH,etal.Automaticembroiderytexturesynthesisforgarmentdesign
andonlinedisplay[J].TheVisualComputer,2021,37:2553-2565.
[2]LiuY,WrightJ,AlvaradoA.MakingBeautifulEmbroideryfor“Frozen2”[C].Special
InterestGrouponComputerGraphicsandInteractiveTechniquesConferenceTalks.2020:
1-2.
[3]BegMA,YuJY.Generatingembroiderypatternsusingimage-to-imagetranslation[J].
arXivpreprintarXiv:2003.02909,2020.
[4]WeiZ,KoYC.Segmentationandsynthesisofembroideryartimagesbasedondeep
learningconvolutionalneuralnetworks[J].InternationalJournalofPatternRecognition
andArtificialIntelligence,2022,36(11):2252018.
[5]HuangX,LiuMY,BelongieS,etal.Multimodalunsupervisedimage-to-image
translation[C].ProceedingsoftheEuropeanconferenceoncomputervision(ECCV).
2018:172-189.
[6]KimT,ChaM,KimH,etal.Learningtodiscovercross-domainrelationswithgenerative
adversarialnetworks[C].Internationalconferenceonmachinelearning.PMLR,2017:
1857-1865.
[7]LiuMY,BreuelT,KautzJ.Unsupervisedimage-to-imagetranslationnetworks[J].
Advancesinneuralinformationprocessingsystems,2017:1–9.
[8]RichardsonE,AlalufY,PatashnikO,etal.Encodinginstyle:astyleganencoderfor
image-to-imagetranslation[C].ProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition.2021:2287-2296.
[9]TangH,LiuH,XuD,etal.Attentiongan:Unpairedimage-to-imagetranslationusing
attention-guidedgenerativeadversarialnetworks[J].IEEEtransactionsonneural
networksandlearningsystems,2021:1–16.
[10]ChenX,McCoolM,KitamotoA,etal.Embroiderymodelingandrendering[M].
ProceedingsofGraphicsInterface2012.2012:131-139.
[11]CuiD,ShengY,ZhangG.Image‐basedembroiderymodelingandrendering[J].
ComputerAnimationandVirtualWorlds,2017,28(2):e1725.1–12.
[12]ShenQ,CuiD,ShengY,etal.Illumination-preservingembroiderysimulationfor
non-photorealisticrendering[C].MultiMediaModeling:23rdInternationalConference,
MMM2017,Reykjavik,Iceland,January4-6,2017,Proceedings,PartII23.Springer
InternationalPublishing,2017:233-244.
[13]YangK,ZhouJ,SunZ,etal.Image-basedirregularneedlingembroideryrendering[C].
proceedingsofthe5thinternationalsymposiumonvisualinformationcommunication
andinteraction.2012:87-94.
[14]ZhouJ,SunZ,YangK.Acontrollablestitchlayoutstrategyforrandomneedle

武汉纺织大学硕士学位论文
42embroidery[J].JournalofZhejiangUniversitySCIENCEC,2014,15(9):729-743.
[15]YangK,SunZ,MaC,etal.Paintwithstitches:Arandom-needleembroideryrendering
method[M].Proceedingsofthe33rdcomputergraphicsInternational.2016:9-12.
[16]YangK,SunZ.Paintwithstitches:astyledefinitionandimage-basedrenderingmethod
forrandom-needleembroidery[J].MultimediaToolsandApplications,2018,77:
12259-12292.
[17]QianW,XuD,CaoJ,etal.Aestheticartsimulationforembroiderystyle[J].Multimedia
ToolsandApplications,2019,78:995-1016.
[18]MaC,SunZ.StitchGeneration:Modelingandcreationofrandom-needleembroidery
basedonMarkovchainmodel[J].MultimediaToolsandApplications,2019,78:
34065-34094.
[19]TakahashiY,FukusatoT.Stitch:Aninteractivedesignsystemforhand-sewn
embroidery[M].ACMSIGGRAPH2018Posters.2018:1-2.
[20]QianW,CaoJ,XuD,etal.Cnn-basedembroiderystylerendering[J].International
JournalofPatternRecognitionandArtificialIntelligence,2020,34(14):2059045.
[21]GoodfellowI,Pouget-AbadieJ,MirzaM,etal.Generativeadversarialnetworks[J].
CommunicationsoftheACM,2020,63(11):139-144.
[22]DumoulinV,BelghaziI,PooleB,etal.Adversariallylearnedinference[J].arXivpreprint
arXiv:1606.00704,2016:1–18.
[23]IsolaP,ZhuJY,ZhouT,etal.Image-to-imagetranslationwithconditionaladversarial
networks[C].ProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition.2017:1125-1134.
[24]MirzaM,OsinderoS.Conditionalgenerativeadversarialnets[J].arXivpreprint
arXiv:1411.1784,2014:1–7.
[25]ZhuJY,ParkT,IsolaP,etal.Unpairedimage-to-imagetranslationusingcycle-consistent
adversarialnetworks[C].ProceedingsoftheIEEEinternationalconferenceoncomputer
vision.2017:2223-2232.
[26]KarrasT,AittalaM,LaineS,etal.Alias-FreeGenerativeAdversarialNetworks[J].
AdvancesinNeuralInformationProcessingSystems,2021,34:852-863.
[27]KarrasT,LaineS,AittalaM,etal.Analyzingandimprovingtheimagequalityof
stylegan[C].ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition.2020:8110-8119.
[28]KingmaDP,WellingM.Auto-EncodingVariationalBayes[J].InternationalConference
onLearningRepresentations,2014,pp.1–14.
[29]LarsenABL,SønderbySK,LarochelleH,etal.Autoencodingbeyondpixelsusinga
learnedsimilaritymetric[C].Internationalconferenceonmachinelearning.PMLR,2016:
1558-1566.
[30]SohnK,LeeH,YanX.Learningstructuredoutputrepresentationusingdeepconditional

参考文献
43generativemodels[J].Advancesinneuralinformationprocessingsystems,
2015:3483–3491.
[31]BaoJ,ChenD,WenF,etal.CVAE-GAN:fine-grainedimagegenerationthrough
asymmetrictraining[C].ProceedingsoftheIEEEinternationalconferenceoncomputer
vision.2017:2745-2754.
[32]LeeHY,TsengHY,MaoQ,etal.Drit++:Diverseimage-to-imagetranslationvia
disentangledrepresentations[J].InternationalJournalofComputerVision,2020,128:
2402-2417.
[33]XuD,WangW,TangH,etal.Structuredattentionguidedconvolutionalneuralfieldsfor
monoculardepthestimation[C].ProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition.2018:3917-3925.
[34]LiangX,ZhangH,XingEP.Generativesemanticmanipulationwithcontrastinggan[J].
ProceedingsoftheEuropeanconferenceoncomputervision(ECCV),2018.
[35]SunR,HuangC,ZhuH,etal.Mask-awarephotorealisticfacialattributemanipulation[J].
ComputationalVisualMedia,2021,7(3):363-374.
[36]MoS,ChoM,ShinJ.Instagan:Instance-awareimage-to-imagetranslation[J].ICLR,
2019.
[37]ChenX,XuC,YangX,etal.Attention-ganforobjecttransfigurationinwildimages[C].
ProceedingsoftheEuropeanconferenceoncomputervision(ECCV).2018:164-180.
[38]KastaniotisD,NtinouI,TsourounisD,etal.Attention-awaregenerativeadversarial
networks(ATA-GANs)[C].2018IEEE13thImage,Video,andMultidimensionalSignal
ProcessingWorkshop(IVMSP).IEEE,2018:1-5.
[39]YangC,KimT,WangR,etal.Show,attend,andtranslate:Unsupervisedimage
translationwithself-regularizationandattention[J].IEEETransactionsonImage
Processing,2019,28(10):4845-4856.
[40]ZhangH,GoodfellowI,MetaxasD,etal.Self-attentiongenerativeadversarial
networks[C].Internationalconferenceonmachinelearning.PMLR,2019:7354-7363.
[41]KimJ,KimM,KangH,etal.U-GAT-IT:UnsupervisedGenerativeAttentionalNetworks
withAdaptiveLayer-InstanceNormalizationforImage-to-ImageTranslation[C].
InternationalConferenceonLearningRepresentations,2020.
[42]AlamiMejjatiY,RichardtC,TompkinJ,etal.Unsupervisedattention-guided
image-to-imagetranslation[J].Advancesinneuralinformationprocessingsystems,2018,
31.
[43]YiZ,ZhangH,TanP,etal.Dualgan:Unsupervisedduallearningforimage-to-image
translation[C].ProceedingsoftheIEEEinternationalconferenceoncomputervision.
2017:2849-2857.
[44]ChoiY,ChoiM,KimM,etal.Stargan:Unifiedgenerativeadversarialnetworksfor
multi-domainimage-to-imagetranslation[C].ProceedingsoftheIEEEconferenceon

武汉纺织大学硕士学位论文
44computervisionandpatternrecognition.2018:8789-8797.
[45]TangH,XuD,WangW,etal.Dualgeneratorgenerativeadversarialnetworksfor
multi-domainimage-to-imagetranslation[C].ComputerVision–ACCV2018:14thAsian
ConferenceonComputerVision,Perth,Australia,December2–6,2018,RevisedSelected
Papers,PartI.Cham:SpringerInternationalPublishing,2019:3-21.
[46]LiB,QiX,LukasiewiczT,etal.Controllabletext-to-imagegeneration[J].Advancesin
NeuralInformationProcessingSystems,2019,32.
[47]YuX,ChenY,LiuS,etal.Multi-mappingimage-to-imagetranslationvialearning
disentanglement[J].AdvancesinNeuralInformationProcessingSystems,2019,32.
[48]MaL,JiaX,SunQ,etal.Poseguidedpersonimagegeneration[J].Advancesinneural
informationprocessingsystems,2017,30.
[49]TangH,WangW,XuD,etal.Gestureganforhandgesture-to-gesturetranslationinthe
wild[C].Proceedingsofthe26thACMinternationalconferenceonMultimedia.2018:
774-782.
[50]DongH,LiangX,GongK,etal.Soft-gatedwarping-ganforpose-guidedpersonimage
synthesis[J].Advancesinneuralinformationprocessingsystems,2018,31.
[51]WangTC,LiuMY,TaoA,etal.Few-shotvideo-to-videosynthesis[J].Advancesin
neuralinformationprocessingsystems,2019.
[52]TangH,XuD,SebeN,etal.Multi-channelattentionselectionganwithcascaded
semanticguidanceforcross-viewimagetranslation[C].ProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition.2019:2417-2426.
[53]RegmiK,BorjiA.Cross-viewimagesynthesisusingconditionalgans[C].Proceedingsof
theIEEEconferenceonComputerVisionandPatternRecognition.2018:3501-3510.
[54]TangH,XuD,YanY,etal.Localclass-specificandglobalimage-levelgenerative
adversarialnetworksforsemantic-guidedscenegeneration[C].Proceedingsofthe
IEEE/CVFconferenceoncomputervisionandpatternrecognition.2020:7870-7879.
[55]AlBaharB,HuangJB.Guidedimage-to-imagetranslationwithbi-directionalfeature
transformation[C].ProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision.2019:9016-9025.
[56]WangM,YangGY,LiR,etal.Example-guidedstyle-consistentimagesynthesisfrom
semanticlabeling[C].ProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition.2019:1495-1504.
[57]XuD,WangW,TangH,etal.Structuredattentionguidedconvolutionalneuralfieldsfor
monoculardepthestimation[C].ProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition.2018:3917-3925.
[58]VaswaniA,ShazeerN,ParmarN,etal.Attentionisallyouneed[J].Advancesinneural
informationprocessingsystems,2017,30.
[59]金志凌，朱鸿雨，苏玉兰，等.基于多粒度交互推理的答案选择方法研究[J].中文

参考文献
45信息学报,2023,37(1):104-111,120.
[60]ZhangH,GoodfellowI,MetaxasD,etal.Self-attentiongenerativeadversarial
networks[C].Internationalconferenceonmachinelearning.PMLR,2019:7354-7363.
[61]YeJ,JiY,SongJ,etal.TowardsEnd-to-EndEmbroideryStyleGeneration:APaired
DatasetandBenchmark[C].PatternRecognitionandComputerVision:4thChinese
Conference,PRCV2021,Beijing,China,October29–November1,2021,Proceedings,
PartIV4.SpringerInternationalPublishing,2021:201-213.
[62]GoldbergBJ,TsonisA.Methodformodifyingembroiderydesignprograms:U.S.Patent
5,270,939[P].1993-12-14.
[63]KyprianidisJE,CollomosseJ,WangT,etal.Stateofthe"art”:Ataxonomyofartistic
stylizationtechniquesforimagesandvideo[J].IEEEtransactionsonvisualizationand
computergraphics,2012,19(5):866-885.
[64]YiR,XiaM,LiuYJ,etal.Linedrawingsforfaceportraitsfromphotosusingglobaland
localstructurebasedGANs[J].IEEETransactionsonPatternAnalysisandMachine
Intelligence,2020,43(10):3462-3475.
[65]ShuY,YiR,XiaM,etal.Gan-basedmulti-stylephotocartoonization[J].IEEE
TransactionsonVisualizationandComputerGraphics,2021,28(10):3376-3390.
[66]ShiY,DebD,JainAK.Warpgan:Automaticcaricaturegeneration[C].Proceedingsofthe
IEEE/CVFconferenceoncomputervisionandpatternrecognition.2019:10762-10771.
[67]TianY,RenJ,ChaiM,etal.Agoodimagegeneratoriswhatyouneedforhigh-resolution
videosynthesis[J].arXivpreprintarXiv:2104.15069,2021.
[68]XiongW,LuoW,MaL,etal.Learningtogeneratetime-lapsevideosusingmulti-stage
dynamicgenerativeadversarialnetworks[C].ProceedingsoftheIEEEConferenceon
ComputerVisionandPatternRecognition.2018:2364-2373.
[69]SaitoM,MatsumotoE,SaitoS.Temporalgenerativeadversarialnetswithsingularvalue
clipping[C].ProceedingsoftheIEEEinternationalconferenceoncomputervision.2017:
2830-2839.
[70]YuL,ZhangW,WangJ,etal.Seqgan:Sequencegenerativeadversarialnetswithpolicy
gradient[C].ProceedingsoftheAAAIconferenceonartificialintelligence.2017,31(1).
[71]LinK,LiD,HeX,etal.Adversarialrankingforlanguagegeneration[J].Advancesin
neuralinformationprocessingsystems,2017:3155-3165.
[72]GuoJ,LuS,CaiH,etal.Longtextgenerationviaadversarialtrainingwithleaked
information[C].ProceedingsoftheAAAIconferenceonartificialintelligence.2018,
32(1).
[73]JhamtaniH,Berg-KirkpatrickT.Modelingself-repetitioninmusicgenerationusing
structuredadversaries[C].Proc.MachineLearningforMediaDiscoveryWorkshop,
extendedabstract.2019.
[74]LiuHM,YangYH.Leadsheetgenerationandarrangementbyconditionalgenerative

武汉纺织大学硕士学位论文
46adversarialnetwork[C].201817thIEEEInternationalConferenceonMachineLearning
andApplications(ICMLA).IEEE,2018:722-727.
[75]CífkaO,ŞimşekliU,RichardG.SupervisedSymbolicMusicStyleTranslationUsing
SyntheticData[C].20thInternationalSocietyforMusicInformationRetrievalConference
(ISMIR).2019.
[76]RichardsonE,AlalufY,PatashnikO,etal.Encodinginstyle:astyleganencoderfor
image-to-imagetranslation[C].ProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition.2021:2287-2296.
[77]HuangX,BelongieS.Arbitrarystyletransferinreal-timewithadaptiveinstance
normalization[C].ProceedingsoftheIEEEinternationalconferenceoncomputervision.
2017:1501-1510.
[78]PaszkeA,GrossS,ChintalaS,etal.Automaticdifferentiationinpytorch[J].NeurIPS
Workshop,pp.1–4,2017.
[79]ZhangR,IsolaP,EfrosAA,etal.Theunreasonableeffectivenessofdeepfeaturesasa
perceptualmetric[C].ProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition.2018:586-595.
[80]DowsonDC,LandauBV.TheFréchetdistancebetweenmultivariatenormal
distributions[J].Journalofmultivariateanalysis,1982,12(3):450-455.
[81]RombachR,BlattmannA,LorenzD,etal.High-resolutionimagesynthesiswithlatent
diffusionmodels[C].ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition.2022:10684-10695.
[82]MildenhallB,SrinivasanPP,TancikM,etal.Nerf:Representingscenesasneural
radiancefieldsforviewsynthesis[J].CommunicationsoftheACM,2021,65(1):99-106.

